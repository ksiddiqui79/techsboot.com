{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kawish Siddiqui Email Link | LinkedIn document.getElementById(&ldquo;emlspn&rdquo;).innerHTML = &ldquo;<a href='mailto:kawish_siddiqui@hotmail.com?subject=Connecting you through \" + window.location.hostname +\"'> Email Me</a>&ldquo;; Background With 19+ years\u2019 dynamic IT experience. Certified and experienced in working, leading and managing teams of engineers for Data Warehouse design, development, administration in massively parallel processing (MPP) architecture and reporting along with solid knowledge of IT Service Management. Primarily possessing customer-obsessed, detail and deadline oriented, innovative attitude along with strong communication, analytical and problem solving skills. Core technical expertise: Teradata Data Warehouse design, development and implementation Experienced of AWS Services for Big Data, Data Lake and Data Warehousing (i.e. Amazon Redshift, Redshift Spectrum, AWS Glue, AWS Athena, Amazon RDS, computing (EC2), Route53 and Storage (S3, Glacier) etc. Automation and reporting using MS Excel VBA macros, Batch and Shell scripting Specialties: AWS Certified Architect \u2013 Big Data AWS Certified Solution Architect \u2013 Associate Teradata Certified Master (V2R5) Teradata Certified Professional Teradata Certified SQL Specialist Teradata Certified Application Developer Teradata Certified Implementation Specialist Teradata Certified Administrator Teradata Certified Designer Project Management Professional (PMI Certified) ITIL v3 Operational Support & Analysis ITIL v3 Planning, Protection & Optimization ITIL v3 Foundation Certified ISO/IEC 20000-1:2005 Lead Auditor (Training) Organizations worked for: Amazon Web Services Xtivia Inc. Teradata Croporation Information Architects Sir Syed University of Engg & Technology","title":"About Me"},{"location":"#background","text":"With 19+ years\u2019 dynamic IT experience. Certified and experienced in working, leading and managing teams of engineers for Data Warehouse design, development, administration in massively parallel processing (MPP) architecture and reporting along with solid knowledge of IT Service Management. Primarily possessing customer-obsessed, detail and deadline oriented, innovative attitude along with strong communication, analytical and problem solving skills.","title":"Background"},{"location":"#core-technical-expertise","text":"Teradata Data Warehouse design, development and implementation Experienced of AWS Services for Big Data, Data Lake and Data Warehousing (i.e. Amazon Redshift, Redshift Spectrum, AWS Glue, AWS Athena, Amazon RDS, computing (EC2), Route53 and Storage (S3, Glacier) etc. Automation and reporting using MS Excel VBA macros, Batch and Shell scripting","title":"Core technical expertise:"},{"location":"#specialties","text":"AWS Certified Architect \u2013 Big Data AWS Certified Solution Architect \u2013 Associate Teradata Certified Master (V2R5) Teradata Certified Professional Teradata Certified SQL Specialist Teradata Certified Application Developer Teradata Certified Implementation Specialist Teradata Certified Administrator Teradata Certified Designer Project Management Professional (PMI Certified) ITIL v3 Operational Support & Analysis ITIL v3 Planning, Protection & Optimization ITIL v3 Foundation Certified ISO/IEC 20000-1:2005 Lead Auditor (Training)","title":"Specialties:"},{"location":"#organizations-worked-for","text":"Amazon Web Services Xtivia Inc. Teradata Croporation Information Architects Sir Syed University of Engg & Technology","title":"Organizations worked for:"},{"location":"blogs/aws/dl_form_aws/","text":"Disclaimer: var git_user = \"dhawalkp\"; var git_tree = \"\" var git_repo_name = \"datalake\"; var git_repo_path = git_user + \"/datalake\"; // // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='https://\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk and <b>should not be considered as Official sources by any mean through this website.<b></i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line; Important This code must not be used for production use as is. This sample is developed only to educate the users in using some of the features of the AWS in developing a ETL Pipeline using AWS Glue and other services. This is my personal github location and is maintained solely by me. This is not the official AWS Open source repository. DataLake Formation in AWS A Data lake contains all data, both raw sources over extended periods of time as well as any processed data. They enable users across multiple business units to refine, explore and enrich data on their terms. Also, enables multiple data access patterns across a shared infrastructure: batch, interactive, online, search, in-memory and other processing engines. AWS Glue provides a serverless Spark-based data processing service. Glue is based on open source frameworks like Apache Spark and the Hive Metastore. This allows our users to go beyond the traditional ETL use cases into more data prep and data processing spanning data exploration, data science, and ofcourse data prep for analytics. Glue helps users do three things \u2013 Discover and understand their data by cataloguing it into a central metadata Catalog, offering libraries and tools to efficiently develop their data prep code, and then run it at scale on a serverless, full managed environment. The backbone storage service for Data Lake best suited is AWS S3. The native features of S3 are exactly what you want from a Data Lake - Replication across AZ\u2019s for high availability and durability, Massively parallel, scalable (Storage scales independent of compute) and Low storage cost. High Level Data Lake Architecture Building a Data Lake from a Relational Data Store This repository contains the sample reference implementation to create data lake from Relational Database services as one of the sources. Source RDS Customer Table Schema can be created by crawler through AWS Glue Crawlers Sample Full Log Customer record format generated by DMS: The sample cdc log file is in data/Tier-1/Customer/Full/ folder structure Sample CDC Log Customer Record format generated by DMS The sample cdc log file is in repo folder - \u201c/data/Tier-1/Customer/CDC/YYYY/MM/DD\u201d folder structure assuming the data are initially partitioned based on the day Please notice the additional OP column in CDC log file added by DMS to tag the type of change. The implementation consists of Hydrating the Data Lake AWS RDS backed by Oracle DB engine integrating with AWS DMS service generating Full and CDC Log files and storing the files on Tier-1 S3 Bucket. Alternatively, the Tier-1 bucket can be hydrated by periodic export process that dumps all the changes as well. The Tier-1 bucket in this example is partitioned based on year/month/day/hour Creating Glue Data Catalog of Tier-1 Bucket for processing AWS Glue Crawlers needs to be configured in order to process CDC and Full Log files in the tier-1 bucket and create data catalog for both. In this case, the Tier-1 Database in Glue will consist of 2 tables i.e. CDC and Full Glue ETL Job for Tier-2 Data Tier-2 ETL job will re-partition based on required keys and hydrate the tier-2 buckets in S3 with S3 Objects based on the partition keys. The script is available in this repo with name - /scripts/DMS_CDC_Tier2_Repartitioning_CustomerID.py and /scripts/DMS_FullLog_Tier2_Repartitioning_CustomerID.py The Tier-2 S3 bucket will eventually have Partitions consisting of multiple versions of S3 Objects for same key. Compaction ETL Job The Compaction job i.e. /scripts/Compaction.py basically deletes the older versions of Objects for all the partitions in tier-2 bucket. In this example, the script uses the last modified time stamp of S3 Object to build the compaction logic. Please refer to additional considerations section below in this documentation for alternative approaches to preserve the consistency. Detailed Data Lake Formation Architecture Detailed ELT Pipeline Best Practices and Performance Considerations One of the key design decisions in making the solution performant will be the selection of appropriate partition keys for target S3 buckets. Please read Working with Partitioned Data in AWS Glue . Partitioning the Source and Target Buckets via Relevant Partition Keys and making use of it in avoiding cross partition joins or full scans. Use Parquet or ORC and S3 Paths for formatting and organizing the data as per partition keys with Compression like Snappy/Gzip formats Use Glue DynamicFrames and make use of PushDownPRedicates based on Partitions to improve/reduce on GETs/PUTs to S3 Use applyMapping wherever possible to restrict the columns In Writing the dynamic frames into partitioned sinks, try to use additional Partitionkeys option so that you can directly write it from DynamicFrame instead of doing intermediate conversion into Spark DataFrame. Use Compaction Technique periodically to delete the old objects for the same key. DMS CDC/Full Load files contains timestamp in the name and should be used to process the data based on this. Care must be taken to make sure multiple CDC Records for same key are not processed in parallel to avoid Data Consistency issues during CDC records consolidation phase. Assumptions & Additional Considerations Consistency and Ordering Multiple changes on the same record can appear in a CDC file generated by AWS DMS.Though, CDC guarantees the files delivered to S3 are in order. The transaction integrity can be maintained as long as you follow the ordering of changes in the file and across sequence of files. The recommendation will be to also have Modification Time Stamp embedded in the source table so that the consistency can be maintained. The Alternative approach to not having modification time stamp in the source table will be to implement pre-tier-1 ETL job that transforms the CDC records and add a synthetic sequence number. Since S3 updates are eventual consistent, a mechanism needs to be developed to make sure the data being queried are not being mutated at the same time. You have to employ the use of temporary scratch location and then compact in tier-3 bucket for example. There may be delays in S3 Upserts and so reliance on S3 Object Last Modified Time may cause inconsistent results. An alternative approach could be to use the DMS CDC logtimestamp or Update Time Stamp column (if present) of the original data and add as meta data in S3 Object and use that for compaction job. Idempotence The ETL job should be aware of idempotence requirements. Since CDC Records presents the latest copy of entire record, the idempotence should not be a concern for ETL job in this case. Performance Tuning the DMS The solution must be aware of the performance tuning and limits of DMS service as mentioned here","title":"DataLake Formation in AWS"},{"location":"blogs/aws/dl_form_aws/#datalake-formation-in-aws","text":"A Data lake contains all data, both raw sources over extended periods of time as well as any processed data. They enable users across multiple business units to refine, explore and enrich data on their terms. Also, enables multiple data access patterns across a shared infrastructure: batch, interactive, online, search, in-memory and other processing engines. AWS Glue provides a serverless Spark-based data processing service. Glue is based on open source frameworks like Apache Spark and the Hive Metastore. This allows our users to go beyond the traditional ETL use cases into more data prep and data processing spanning data exploration, data science, and ofcourse data prep for analytics. Glue helps users do three things \u2013 Discover and understand their data by cataloguing it into a central metadata Catalog, offering libraries and tools to efficiently develop their data prep code, and then run it at scale on a serverless, full managed environment. The backbone storage service for Data Lake best suited is AWS S3. The native features of S3 are exactly what you want from a Data Lake - Replication across AZ\u2019s for high availability and durability, Massively parallel, scalable (Storage scales independent of compute) and Low storage cost.","title":"DataLake Formation in AWS"},{"location":"blogs/aws/dl_form_aws/#high-level-data-lake-architecture","text":"","title":"High Level Data Lake Architecture"},{"location":"blogs/aws/dl_form_aws/#building-a-data-lake-from-a-relational-data-store","text":"This repository contains the sample reference implementation to create data lake from Relational Database services as one of the sources. Source RDS Customer Table Schema can be created by crawler through AWS Glue Crawlers Sample Full Log Customer record format generated by DMS: The sample cdc log file is in data/Tier-1/Customer/Full/ folder structure Sample CDC Log Customer Record format generated by DMS The sample cdc log file is in repo folder - \u201c/data/Tier-1/Customer/CDC/YYYY/MM/DD\u201d folder structure assuming the data are initially partitioned based on the day Please notice the additional OP column in CDC log file added by DMS to tag the type of change. The implementation consists of","title":"Building a Data Lake from a Relational Data Store"},{"location":"blogs/aws/dl_form_aws/#hydrating-the-data-lake","text":"AWS RDS backed by Oracle DB engine integrating with AWS DMS service generating Full and CDC Log files and storing the files on Tier-1 S3 Bucket. Alternatively, the Tier-1 bucket can be hydrated by periodic export process that dumps all the changes as well. The Tier-1 bucket in this example is partitioned based on year/month/day/hour","title":"Hydrating the Data Lake"},{"location":"blogs/aws/dl_form_aws/#creating-glue-data-catalog-of-tier-1-bucket-for-processing","text":"AWS Glue Crawlers needs to be configured in order to process CDC and Full Log files in the tier-1 bucket and create data catalog for both. In this case, the Tier-1 Database in Glue will consist of 2 tables i.e. CDC and Full","title":"Creating Glue Data Catalog of Tier-1 Bucket for processing"},{"location":"blogs/aws/dl_form_aws/#glue-etl-job-for-tier-2-data","text":"Tier-2 ETL job will re-partition based on required keys and hydrate the tier-2 buckets in S3 with S3 Objects based on the partition keys. The script is available in this repo with name - /scripts/DMS_CDC_Tier2_Repartitioning_CustomerID.py and /scripts/DMS_FullLog_Tier2_Repartitioning_CustomerID.py The Tier-2 S3 bucket will eventually have Partitions consisting of multiple versions of S3 Objects for same key.","title":"Glue ETL Job for Tier-2 Data"},{"location":"blogs/aws/dl_form_aws/#compaction-etl-job","text":"The Compaction job i.e. /scripts/Compaction.py basically deletes the older versions of Objects for all the partitions in tier-2 bucket. In this example, the script uses the last modified time stamp of S3 Object to build the compaction logic. Please refer to additional considerations section below in this documentation for alternative approaches to preserve the consistency.","title":"Compaction ETL Job"},{"location":"blogs/aws/dl_form_aws/#detailed-data-lake-formation-architecture","text":"","title":"Detailed Data Lake Formation Architecture"},{"location":"blogs/aws/dl_form_aws/#detailed-elt-pipeline","text":"","title":"Detailed ELT Pipeline"},{"location":"blogs/aws/dl_form_aws/#best-practices-and-performance-considerations","text":"One of the key design decisions in making the solution performant will be the selection of appropriate partition keys for target S3 buckets. Please read Working with Partitioned Data in AWS Glue . Partitioning the Source and Target Buckets via Relevant Partition Keys and making use of it in avoiding cross partition joins or full scans. Use Parquet or ORC and S3 Paths for formatting and organizing the data as per partition keys with Compression like Snappy/Gzip formats Use Glue DynamicFrames and make use of PushDownPRedicates based on Partitions to improve/reduce on GETs/PUTs to S3 Use applyMapping wherever possible to restrict the columns In Writing the dynamic frames into partitioned sinks, try to use additional Partitionkeys option so that you can directly write it from DynamicFrame instead of doing intermediate conversion into Spark DataFrame. Use Compaction Technique periodically to delete the old objects for the same key. DMS CDC/Full Load files contains timestamp in the name and should be used to process the data based on this. Care must be taken to make sure multiple CDC Records for same key are not processed in parallel to avoid Data Consistency issues during CDC records consolidation phase.","title":"Best Practices and Performance Considerations"},{"location":"blogs/aws/dl_form_aws/#assumptions-additional-considerations","text":"","title":"Assumptions &amp; Additional Considerations"},{"location":"blogs/aws/dl_form_aws/#consistency-and-ordering","text":"Multiple changes on the same record can appear in a CDC file generated by AWS DMS.Though, CDC guarantees the files delivered to S3 are in order. The transaction integrity can be maintained as long as you follow the ordering of changes in the file and across sequence of files. The recommendation will be to also have Modification Time Stamp embedded in the source table so that the consistency can be maintained. The Alternative approach to not having modification time stamp in the source table will be to implement pre-tier-1 ETL job that transforms the CDC records and add a synthetic sequence number. Since S3 updates are eventual consistent, a mechanism needs to be developed to make sure the data being queried are not being mutated at the same time. You have to employ the use of temporary scratch location and then compact in tier-3 bucket for example. There may be delays in S3 Upserts and so reliance on S3 Object Last Modified Time may cause inconsistent results. An alternative approach could be to use the DMS CDC logtimestamp or Update Time Stamp column (if present) of the original data and add as meta data in S3 Object and use that for compaction job.","title":"Consistency and Ordering"},{"location":"blogs/aws/dl_form_aws/#idempotence","text":"The ETL job should be aware of idempotence requirements. Since CDC Records presents the latest copy of entire record, the idempotence should not be a concern for ETL job in this case.","title":"Idempotence"},{"location":"blogs/aws/dl_form_aws/#performance-tuning-the-dms","text":"The solution must be aware of the performance tuning and limits of DMS service as mentioned here","title":"Performance Tuning the DMS"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/","text":"Advance - Serverless ETL for Amazon Redshift Overview This blog post is in continuation of Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift This post is to show very basic level (100 level) of automation without ETL metadata management and tracking etc. Our next post (Intermediate and Advance) will have details and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples. Architecture Assumptions and Basic Setup Following are assumption and Basic Setup - Amazon Redshift cluster is already setup (see Launch a Sample Amazon Redshift Cluster ) - S3 Structure setup is BucketName -> Redshift Schema Name -> Table Name -> Data File(s) You can change the Lambda logic and functionality based on your S3 structure Setup Amazon Redshift User, Schema and Table -- CREATE Group and User CREATE GROUP grp_rsloader; CREATE USER rsloader_user PASSWORD 'TechsBoot_2019' IN GROUP grp_rs_loader; -- Create Schema CREATE SCHEMA IF NOT EXISTS rsloader; -- Setup Access Rights on Schema for Group ALTER DEFAULT PRIVILEGES IN SCHEMA rsloader GRANT ALL ON TABLES TO GROUP grp_rs_loader; GRANT ALL ON SCHEMA rsloader TO GROUP grp_rs_loader; GRANT ALL ON ALL TABLES IN SCHEMA rsloader TO GROUP grp_rs_loader; -- Create Table CREATE TABLE IF NOT EXISTS rsloader.lineitem ( l_orderkey INTEGER NOT NULL ,l_partkey INTEGER NOT NULL ENCODE lzo ,l_suppkey INTEGER NOT NULL ENCODE lzo ,l_linenumber INTEGER NOT NULL ENCODE lzo ,l_quantity NUMERIC(15,2) NOT NULL ENCODE lzo ,l_extendedprice NUMERIC(15,2) NOT NULL ENCODE lzo ,l_discount NUMERIC(15,2) NOT NULL ENCODE lzo ,l_tax NUMERIC(15,2) NOT NULL ENCODE lzo ,l_returnflag VARCHAR(1) NOT NULL ENCODE lzo ,l_linestatus VARCHAR(1) NOT NULL ENCODE lzo ,l_shipdate DATE NOT NULL ENCODE lzo ,l_commitdate DATE NOT NULL ENCODE lzo ,l_receiptdate DATE NOT NULL ENCODE lzo ,l_shipinstruct VARCHAR(25) NOT NULL ENCODE lzo ,l_shipmode VARCHAR(10) NOT NULL ENCODE lzo ,l_comment VARCHAR(44) NOT NULL ENCODE lzo ) DISTSTYLE KEY DISTKEY (l_orderkey) SORTKEY ( l_orderkey ) ; Setup S3 bucket and hierarchy Create S3 Bucket (we will use techsboot_rsloader S3 bucket name in our examples) Create folder rsloader in S3 Bucket Create a subfolder lineitem in rsloader folder S3 structure will look like techsboot_rsloader (or your bucket name) |__ rsloader |__ lineitem Create IAM Role for AWS Glue Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Glue as the service that will use this role Click Next: Permissions button at the bottom Select AWSGlueServiceRole Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role Create IAM Role for AWS Lambda Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Lambda as the service that will use this role Click Next: Permissions button at the bottom Select AWSLambdaFullAccess Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role Create AWS Glue Job Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() # print(REDSHIFT_COPY_STATEMENT) cursor.execute(REDSHIFT_COPY_STATEMENT) # result = cursor.fetchone() cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1) Create AWS Lambda Function Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in previous step Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in previous step Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') # def lambda_handler(event, context): for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"wfm-poc-copy-from-lambda\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Printing Debugging Message print (fullS3Message) print (\"fullS3Path = \" + fullS3Path) print (\"Bucket = \" + bucket) print (\"Key = \" + key) print (\"schema_name = \" + schema_name) print (\"table_name = \" + table_name) print (\"filename = \" + filename) # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") Pre-load testing Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL, result of this SQL should be zero (0) . sql SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Upload sample data into S3 Download sample data (2 files) from following links File 1 File 2 Go to https://console.aws.amazon.com/s3/ Click on bucket you created in previous step Click on rsloader folder Click on lineitem subfolder Click on Upload button Click Add on displayed window In file selection dialog, select files you downloaded on step 1 Click Next , and then Next , and then again Next (by keeping next 2 steps of displayed windows as default) Click Upload Verify data in Amazon Redshift Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Result should be like below table l_linestatus count R 200 C 200","title":"Advance - Serverless ETL for Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#advance-serverless-etl-for-amazon-redshift","text":"","title":"Advance - Serverless ETL for Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#overview","text":"This blog post is in continuation of Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift This post is to show very basic level (100 level) of automation without ETL metadata management and tracking etc. Our next post (Intermediate and Advance) will have details and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples.","title":"Overview"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#architecture","text":"","title":"Architecture"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#assumptions-and-basic-setup","text":"Following are assumption and Basic Setup - Amazon Redshift cluster is already setup (see Launch a Sample Amazon Redshift Cluster ) - S3 Structure setup is BucketName -> Redshift Schema Name -> Table Name -> Data File(s) You can change the Lambda logic and functionality based on your S3 structure","title":"Assumptions and Basic Setup"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#setup-amazon-redshift-user-schema-and-table","text":"-- CREATE Group and User CREATE GROUP grp_rsloader; CREATE USER rsloader_user PASSWORD 'TechsBoot_2019' IN GROUP grp_rs_loader; -- Create Schema CREATE SCHEMA IF NOT EXISTS rsloader; -- Setup Access Rights on Schema for Group ALTER DEFAULT PRIVILEGES IN SCHEMA rsloader GRANT ALL ON TABLES TO GROUP grp_rs_loader; GRANT ALL ON SCHEMA rsloader TO GROUP grp_rs_loader; GRANT ALL ON ALL TABLES IN SCHEMA rsloader TO GROUP grp_rs_loader; -- Create Table CREATE TABLE IF NOT EXISTS rsloader.lineitem ( l_orderkey INTEGER NOT NULL ,l_partkey INTEGER NOT NULL ENCODE lzo ,l_suppkey INTEGER NOT NULL ENCODE lzo ,l_linenumber INTEGER NOT NULL ENCODE lzo ,l_quantity NUMERIC(15,2) NOT NULL ENCODE lzo ,l_extendedprice NUMERIC(15,2) NOT NULL ENCODE lzo ,l_discount NUMERIC(15,2) NOT NULL ENCODE lzo ,l_tax NUMERIC(15,2) NOT NULL ENCODE lzo ,l_returnflag VARCHAR(1) NOT NULL ENCODE lzo ,l_linestatus VARCHAR(1) NOT NULL ENCODE lzo ,l_shipdate DATE NOT NULL ENCODE lzo ,l_commitdate DATE NOT NULL ENCODE lzo ,l_receiptdate DATE NOT NULL ENCODE lzo ,l_shipinstruct VARCHAR(25) NOT NULL ENCODE lzo ,l_shipmode VARCHAR(10) NOT NULL ENCODE lzo ,l_comment VARCHAR(44) NOT NULL ENCODE lzo ) DISTSTYLE KEY DISTKEY (l_orderkey) SORTKEY ( l_orderkey ) ;","title":"Setup Amazon Redshift User, Schema and Table"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#setup-s3-bucket-and-hierarchy","text":"Create S3 Bucket (we will use techsboot_rsloader S3 bucket name in our examples) Create folder rsloader in S3 Bucket Create a subfolder lineitem in rsloader folder S3 structure will look like techsboot_rsloader (or your bucket name) |__ rsloader |__ lineitem","title":"Setup S3 bucket and hierarchy"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#create-iam-role-for-aws-glue","text":"Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Glue as the service that will use this role Click Next: Permissions button at the bottom Select AWSGlueServiceRole Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role","title":"Create IAM Role for AWS Glue"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#create-iam-role-for-aws-lambda","text":"Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Lambda as the service that will use this role Click Next: Permissions button at the bottom Select AWSLambdaFullAccess Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role","title":"Create IAM Role for AWS Lambda"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#create-aws-glue-job","text":"Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() # print(REDSHIFT_COPY_STATEMENT) cursor.execute(REDSHIFT_COPY_STATEMENT) # result = cursor.fetchone() cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1)","title":"Create AWS Glue Job"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#create-aws-lambda-function","text":"Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in previous step Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in previous step Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') # def lambda_handler(event, context): for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"wfm-poc-copy-from-lambda\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Printing Debugging Message print (fullS3Message) print (\"fullS3Path = \" + fullS3Path) print (\"Bucket = \" + bucket) print (\"Key = \" + key) print (\"schema_name = \" + schema_name) print (\"table_name = \" + table_name) print (\"filename = \" + filename) # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")","title":"Create AWS Lambda Function"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#pre-load-testing","text":"Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL, result of this SQL should be zero (0) . sql SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R');","title":"Pre-load testing"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#upload-sample-data-into-s3","text":"Download sample data (2 files) from following links File 1 File 2 Go to https://console.aws.amazon.com/s3/ Click on bucket you created in previous step Click on rsloader folder Click on lineitem subfolder Click on Upload button Click Add on displayed window In file selection dialog, select files you downloaded on step 1 Click Next , and then Next , and then again Next (by keeping next 2 steps of displayed windows as default) Click Upload","title":"Upload sample data into S3"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift-WIP/#verify-data-in-amazon-redshift","text":"Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Result should be like below table l_linestatus count R 200 C 200","title":"Verify data in Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift/","text":"Important This code must not be used for production use as is. This sample is developed only to educate the users in using some of the features of the AWS in developing Serverless ETL for Redshift Advance - Serverless ETL for Amazon Redshift Coming Soon","title":"Advance"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift/#advance-serverless-etl-for-amazon-redshift","text":"","title":"Advance - Serverless ETL for Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/advsrvlsglueredshift/#coming-soon","text":"","title":"Coming Soon"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/","text":"Important This code must not be used for production use as is. This sample is developed only to educate the users in using some of the features of the AWS in developing Serverless ETL for Redshift Beginners - Serverless ETL for Amazon Redshift Overview This blog post is the first (Beginners Level) postof Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift ( Coming Soon ) This blog post is to help Data Warehousing professionals who are starting learning AWS Services and intrested in basic serverless architecture for ETL/ELT process. This post is to show very basic level (100 level) of automation without ETL metadata management and tracking etc. Our next post (Intermediate and Advance) will have details and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples. Services Used Amazon S3 AWS Lambda AWS Glue Amazon Redshift Architecture Assumptions and Basic Setup Following are assumption and Basic Setup Amazon Redshift cluster is already setup (see Launch a Sample Amazon Redshift Cluster ) S3 Structure setup is BucketName -> Redshift Schema Name -> Table Name -> Data File(s) You can change the Lambda logic and functionality based on your S3 structure Setup Amazon Redshift User, Schema and Table -- CREATE Group and User CREATE GROUP grp_rsloader; CREATE USER rsloader_user PASSWORD 'TechsBoot_2019' IN GROUP grp_rs_loader; -- Create Schema CREATE SCHEMA IF NOT EXISTS rsloader; -- Setup Access Rights on Schema for Group ALTER DEFAULT PRIVILEGES IN SCHEMA rsloader GRANT ALL ON TABLES TO GROUP grp_rs_loader; GRANT ALL ON SCHEMA rsloader TO GROUP grp_rs_loader; GRANT ALL ON ALL TABLES IN SCHEMA rsloader TO GROUP grp_rs_loader; -- Create Table CREATE TABLE IF NOT EXISTS rsloader.lineitem ( l_orderkey INTEGER NOT NULL ,l_partkey INTEGER NOT NULL ENCODE lzo ,l_suppkey INTEGER NOT NULL ENCODE lzo ,l_linenumber INTEGER NOT NULL ENCODE lzo ,l_quantity NUMERIC(15,2) NOT NULL ENCODE lzo ,l_extendedprice NUMERIC(15,2) NOT NULL ENCODE lzo ,l_discount NUMERIC(15,2) NOT NULL ENCODE lzo ,l_tax NUMERIC(15,2) NOT NULL ENCODE lzo ,l_returnflag VARCHAR(1) NOT NULL ENCODE lzo ,l_linestatus VARCHAR(1) NOT NULL ENCODE lzo ,l_shipdate DATE NOT NULL ENCODE lzo ,l_commitdate DATE NOT NULL ENCODE lzo ,l_receiptdate DATE NOT NULL ENCODE lzo ,l_shipinstruct VARCHAR(25) NOT NULL ENCODE lzo ,l_shipmode VARCHAR(10) NOT NULL ENCODE lzo ,l_comment VARCHAR(44) NOT NULL ENCODE lzo ) DISTSTYLE KEY DISTKEY (l_orderkey) SORTKEY ( l_orderkey ) ; Setup S3 bucket and hierarchy Create S3 Bucket (we will use techsboot_rsloader S3 bucket name in our examples) Create folder rsloader in S3 Bucket Create a subfolder lineitem in rsloader folder S3 structure will look like techsboot_rsloader (or your bucket name) |__ rsloader |__ lineitem Create IAM Role for AWS Glue Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Glue as the service that will use this role Click Next: Permissions button at the bottom Select AWSGlueServiceRole Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role Create IAM Role for AWS Lambda Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Lambda as the service that will use this role Click Next: Permissions button at the bottom Select AWSLambdaFullAccess Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role Create AWS Glue Job Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() # print(REDSHIFT_COPY_STATEMENT) cursor.execute(REDSHIFT_COPY_STATEMENT) # result = cursor.fetchone() cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1) Create AWS Lambda Function Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in previous step Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in previous step Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') # def lambda_handler(event, context): for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"glu_techsboot_rsloader\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Printing Debugging Message print (fullS3Message) print (\"fullS3Path = \" + fullS3Path) print (\"Bucket = \" + bucket) print (\"Key = \" + key) print (\"schema_name = \" + schema_name) print (\"table_name = \" + table_name) print (\"filename = \" + filename) # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") Pre-load testing Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL, result of this SQL should be zero (0) . sql SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Upload sample data into S3 Download sample data (2 files) from following links File 1 File 2 Go to https://console.aws.amazon.com/s3/ Click on bucket you created in previous step Click on rsloader folder Click on lineitem subfolder Click on Upload button Click Add on displayed window In file selection dialog, select files you downloaded on step 1 Click Next , and then Next , and then again Next (by keeping next 2 steps of displayed windows as default) Click Upload Verify data in Amazon Redshift Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Result should be like below table l_linestatus count R 200 C 200","title":"Beginners"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#beginners-serverless-etl-for-amazon-redshift","text":"","title":"Beginners - Serverless ETL for Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#overview","text":"This blog post is the first (Beginners Level) postof Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift ( Coming Soon ) This blog post is to help Data Warehousing professionals who are starting learning AWS Services and intrested in basic serverless architecture for ETL/ELT process. This post is to show very basic level (100 level) of automation without ETL metadata management and tracking etc. Our next post (Intermediate and Advance) will have details and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples.","title":"Overview"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#services-used","text":"Amazon S3 AWS Lambda AWS Glue Amazon Redshift","title":"Services Used"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#architecture","text":"","title":"Architecture"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#assumptions-and-basic-setup","text":"Following are assumption and Basic Setup Amazon Redshift cluster is already setup (see Launch a Sample Amazon Redshift Cluster ) S3 Structure setup is BucketName -> Redshift Schema Name -> Table Name -> Data File(s) You can change the Lambda logic and functionality based on your S3 structure","title":"Assumptions and Basic Setup"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#setup-amazon-redshift-user-schema-and-table","text":"-- CREATE Group and User CREATE GROUP grp_rsloader; CREATE USER rsloader_user PASSWORD 'TechsBoot_2019' IN GROUP grp_rs_loader; -- Create Schema CREATE SCHEMA IF NOT EXISTS rsloader; -- Setup Access Rights on Schema for Group ALTER DEFAULT PRIVILEGES IN SCHEMA rsloader GRANT ALL ON TABLES TO GROUP grp_rs_loader; GRANT ALL ON SCHEMA rsloader TO GROUP grp_rs_loader; GRANT ALL ON ALL TABLES IN SCHEMA rsloader TO GROUP grp_rs_loader; -- Create Table CREATE TABLE IF NOT EXISTS rsloader.lineitem ( l_orderkey INTEGER NOT NULL ,l_partkey INTEGER NOT NULL ENCODE lzo ,l_suppkey INTEGER NOT NULL ENCODE lzo ,l_linenumber INTEGER NOT NULL ENCODE lzo ,l_quantity NUMERIC(15,2) NOT NULL ENCODE lzo ,l_extendedprice NUMERIC(15,2) NOT NULL ENCODE lzo ,l_discount NUMERIC(15,2) NOT NULL ENCODE lzo ,l_tax NUMERIC(15,2) NOT NULL ENCODE lzo ,l_returnflag VARCHAR(1) NOT NULL ENCODE lzo ,l_linestatus VARCHAR(1) NOT NULL ENCODE lzo ,l_shipdate DATE NOT NULL ENCODE lzo ,l_commitdate DATE NOT NULL ENCODE lzo ,l_receiptdate DATE NOT NULL ENCODE lzo ,l_shipinstruct VARCHAR(25) NOT NULL ENCODE lzo ,l_shipmode VARCHAR(10) NOT NULL ENCODE lzo ,l_comment VARCHAR(44) NOT NULL ENCODE lzo ) DISTSTYLE KEY DISTKEY (l_orderkey) SORTKEY ( l_orderkey ) ;","title":"Setup Amazon Redshift User, Schema and Table"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#setup-s3-bucket-and-hierarchy","text":"Create S3 Bucket (we will use techsboot_rsloader S3 bucket name in our examples) Create folder rsloader in S3 Bucket Create a subfolder lineitem in rsloader folder S3 structure will look like techsboot_rsloader (or your bucket name) |__ rsloader |__ lineitem","title":"Setup S3 bucket and hierarchy"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#create-iam-role-for-aws-glue","text":"Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Glue as the service that will use this role Click Next: Permissions button at the bottom Select AWSGlueServiceRole Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role","title":"Create IAM Role for AWS Glue"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#create-iam-role-for-aws-lambda","text":"Go to https://console.aws.amazon.com/iam Click Roles in left panel and click Create role button Select AWS services in type of trusted entity and Select/Click Lambda as the service that will use this role Click Next: Permissions button at the bottom Select AWSLambdaFullAccess Policy from the list and click Next: Tags Provide Tags\u2019 Key-Values if you want and click Next: Review Enter Role Name and Description and click Create role","title":"Create IAM Role for AWS Lambda"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#create-aws-glue-job","text":"Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() # print(REDSHIFT_COPY_STATEMENT) cursor.execute(REDSHIFT_COPY_STATEMENT) # result = cursor.fetchone() cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1)","title":"Create AWS Glue Job"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#create-aws-lambda-function","text":"Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in previous step Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in previous step Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') # def lambda_handler(event, context): for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"glu_techsboot_rsloader\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Printing Debugging Message print (fullS3Message) print (\"fullS3Path = \" + fullS3Path) print (\"Bucket = \" + bucket) print (\"Key = \" + key) print (\"schema_name = \" + schema_name) print (\"table_name = \" + table_name) print (\"filename = \" + filename) # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")","title":"Create AWS Lambda Function"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#pre-load-testing","text":"Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL, result of this SQL should be zero (0) . sql SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R');","title":"Pre-load testing"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#upload-sample-data-into-s3","text":"Download sample data (2 files) from following links File 1 File 2 Go to https://console.aws.amazon.com/s3/ Click on bucket you created in previous step Click on rsloader folder Click on lineitem subfolder Click on Upload button Click Add on displayed window In file selection dialog, select files you downloaded on step 1 Click Next , and then Next , and then again Next (by keeping next 2 steps of displayed windows as default) Click Upload","title":"Upload sample data into S3"},{"location":"blogs/aws/rssrvlessetl/begsrvlsglueredshift/#verify-data-in-amazon-redshift","text":"Connect to Redshift cluster using SQL Client (i.e. SQLWorkbench or DBeaver) Run following SQL SELECT COUNT(*) FROM rsloader.lineitem WHERE l_linestatus in ('C', 'R'); Result should be like below table l_linestatus count R 200 C 200","title":"Verify data in Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/","text":"Important This code must not be used for production use as is. This sample is developed only to educate the users in using some of the features of the AWS in developing Serverless ETL for Redshift Intermediate - Serverless ETL for Amazon Redshift Overview This blog post is in continuation of Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift ( Coming Soon ) This post is to show some intermediate level of automation with limited ETL metadata management and job tracking etc. Our next post Advance - Serverless ETL for Amazon Redshift will have comprehensive design and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples. Architecture Assumptions Assumption for this post is that following steps are already performed using our previous blog post . Assumptions and Basic Setup Setup Amazon Redshift User, Schema and Table Setup S3 bucket and hierarchy Create IAM Role for AWS Glue Create IAM Role for AWS Lambda Create a DynamoDB table to capture job log Go to https://console.aws.amazon.com/dynamodb Click on Tables in left panel and click on Create table button. Enter following information in relevent text boxes: Table name : DD_JobExecLog Primary partition key : JobName (with String data type) Check the checkbox Add sort key Primary sort key : JobRunID (with String data type) Keep rest as default and click Create button at the bottom of the page. Create SNS topic Go to https://console.aws.amazon.com/sns Click on Topics in left panel and click on Create topic button. Enter Name techsboot-rsloader-notification Enter Display name techsbootSNS Click Create topic button at the bottom of the page Subscribe to SNS topic Go to https://console.aws.amazon.com/sns Click on Topics in left panel and click on the topic techsboot-rsloader-notification . Click Create subscription Leave Topic ARN as default Select Email from Protocol dropdown In Endpoint , enter an email address where notification email should be sent. Click on Create subscription button at the bottom of the page. Note Amazon SNS service will send a varification email to the provided email address before enabling this subscription. So don\u2019t forget to check email and confirm subscription in order to receive notifications from this SNS topic. Create AWS Glue Job Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" # REDSHIFT_QUERY = \"SELECT DISTINCT tablename FROM pg_table_def WHERE schemaname = 'public' ORDER BY tablename\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() cursor.execute(REDSHIFT_COPY_STATEMENT) cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1) Create AWS Lambda Function to invoke Glue Job Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-trigger-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you created in Create IAM Role for AWS Lambda section Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in Setup S3 bucket and hierarchy section Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') dd = boto3.resource('dynamodb') table = dd.Table('DD_JobExecLog') def lambda_handler(event, context): start_timestamp = str(datetime.now()) for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"glu_techsboot_rsloader\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) ########################### # # table.put_item( Item={ 'JobName': glue_job_name, 'JobRunID': parsed_response['JobRunId'], 'job_state': 'STARTED (Lambda)', 'start_timestamp': start_timestamp, 'update_timestamp': 'null', 'job_message': 'Job Triggered by Lambda', 'job_severity': 'null', 's3_file_key': fullS3Path, 'job_region': 'null', 'job_time': 'null', 'job_account': 'null', 'glue_metadata': parsed_response } ) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") Create AWS Lambda Function for Glue job progress and status logging Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-gluelogger-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in Create IAM Role for AWS Lambda section Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button from datetime import datetime, timedelta import json import boto3 dd = boto3.resource('dynamodb') table = dd.Table('DD_JobExecLog') def lambda_handler(event, context): #print(\"Received event: \" + json.dumps(event, indent=2)) jobName = event['detail']['jobName'] jobRunId = event['detail']['jobRunId'] job_time = event['time'] j_account = event['account'] j_region = event['region'] j_severity = event['detail']['severity'] j_state = event['detail']['state'] j_message = event['detail']['message'] update_timestamp = str(datetime.now()) # if jobName == \"glu_techsboot_rsloader\": table.update_item( Key={ 'JobName': jobName, 'JobRunID': jobRunId }, UpdateExpression='SET job_message= :msg, job_severity= :sev, update_timestamp = :upd_ts, job_time= :jb_tm, job_region= :j_region, job_state= :v_state, job_account= :acc ', ExpressionAttributeValues={ ':upd_ts': update_timestamp, ':jb_tm': job_time, ':j_region': j_region, ':sev': j_severity, ':v_state': j_state, ':msg': j_message, ':acc': j_account } ) # print(\"current_timestamp: \" + str(datetime.now())) print(\"account: \" + j_account) print(\"time: \" + job_time) print(\"region: \" + j_region) print(\"jobName: \" + jobName) print(\"severity: \" + j_severity) print(\"state: \" + j_state) print(\"jobRunId: \" + jobRunId) print(\"message: \" + j_message) print(\"-----------------------------\") print(\"From Glue Job Event : \" + json.dumps(event)) Add CloudWatch Event Rule to capture Glue job status Go to https://console.aws.amazon.com/cloudwatch Click Rules in left panel under Events section and click Create rule button In Event Srouce section, click on Event Pattern radio button. From Event Pattern dropdown, select Build custom event pattern and copy/paste follwoing JSON { \"source\": [\"aws.glue\"], \"detail-type\": [ \"Glue Job State Change\", \"Glue Job Run Status\" ] } In Targets ( right side ) section, click Add target Select Lambda function in first dropdown Select techsboot-rsloader-gluelogger-lambda lambda fuction created in previous steps in Function dropdown. Click Add target again to add another target Select SNS Topic in first dropdown Select techsboot-rsloader-notification SNS topic ( created in previous steps ) in Topic dropdown. Click Configure details button at the bottom In Name text box, enter techsboot-rsloader-gluestatus-rule and provide a Description text. Click Create rule button Data loading and validation We are using the following steps for Data loading and validation same as we performed for our Beginner post Pre-load testing Upload sample data into S3 Verify data in Amazon Redshift Validate loggin in DynamoDB Table Go to https://console.aws.amazon.com/dynamodb Click on Tables in left panel and click on DD_JobExecLog to open table. Click on Items tab to see the job status records in DynamoDB table","title":"Intermediate"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#intermediate-serverless-etl-for-amazon-redshift","text":"","title":"Intermediate - Serverless ETL for Amazon Redshift"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#overview","text":"This blog post is in continuation of Serverless ETL for Amazon Redshift series. Beginners - Serverless ETL for Amazon Redshift Intermediate - Serverless ETL for Amazon Redshift Advance - Serverless ETL for Amazon Redshift ( Coming Soon ) This post is to show some intermediate level of automation with limited ETL metadata management and job tracking etc. Our next post Advance - Serverless ETL for Amazon Redshift will have comprehensive design and best practices around Serverless ETL/ELT architecture and step and step guid along with code samples.","title":"Overview"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#architecture","text":"","title":"Architecture"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#assumptions","text":"Assumption for this post is that following steps are already performed using our previous blog post . Assumptions and Basic Setup Setup Amazon Redshift User, Schema and Table Setup S3 bucket and hierarchy Create IAM Role for AWS Glue Create IAM Role for AWS Lambda","title":"Assumptions"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#create-a-dynamodb-table-to-capture-job-log","text":"Go to https://console.aws.amazon.com/dynamodb Click on Tables in left panel and click on Create table button. Enter following information in relevent text boxes: Table name : DD_JobExecLog Primary partition key : JobName (with String data type) Check the checkbox Add sort key Primary sort key : JobRunID (with String data type) Keep rest as default and click Create button at the bottom of the page.","title":"Create a DynamoDB table to capture job log"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#create-sns-topic","text":"Go to https://console.aws.amazon.com/sns Click on Topics in left panel and click on Create topic button. Enter Name techsboot-rsloader-notification Enter Display name techsbootSNS Click Create topic button at the bottom of the page","title":"Create SNS topic"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#subscribe-to-sns-topic","text":"Go to https://console.aws.amazon.com/sns Click on Topics in left panel and click on the topic techsboot-rsloader-notification . Click Create subscription Leave Topic ARN as default Select Email from Protocol dropdown In Endpoint , enter an email address where notification email should be sent. Click on Create subscription button at the bottom of the page. Note Amazon SNS service will send a varification email to the provided email address before enabling this subscription. So don\u2019t forget to check email and confirm subscription in order to receive notifications from this SNS topic.","title":"Subscribe to SNS topic"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#create-aws-glue-job","text":"Go to https://console.aws.amazon.com/glue Click on Jobs in left panel and click on Add job button on main panel Enter Name glu_techsboot_rsloader Select IAM Role from list which was created in previous step Select Python shell in Type Select A new script to be authored by you in This job runs section Enter Script file name Select appropriate S3 path to store glue script in S3 path where the script is stored section Expand Security configuration, script libraries, and job parameters (optional) section In Python library path text box, enter s3://techsboot/py-ref/pg8000.egg Set Max concurrency to 10 to handel 10 file uploads at the same time. Or you can set this number based on your implementation needs. Click Next Click Save job and edit script Copy and Paste following python custom code and Glue script editor see Adding Python Shell Jobs in AWS Glue for detailed step by step job creation guid. Replace correct values for all variables enclosed in < > within the script. import os import sys import boto3 import json from awsglue.utils import getResolvedOptions import pg8000 ## @params: [JOB_NAME] args = getResolvedOptions(sys.argv, ['JOB_NAME', 'schema_name', 'table_name', 's3_file']) schema_name = args['schema_name'] table_name = args['table_name'] s3_file = args['s3_file'] # REDSHIFT_DATABASE = \"<Your_Redshift_Database>\" REDSHIFT_USER = \"rsloader_user\" REDSHIFT_PASSWD = \"TechsBoot_2019\" REDSHIFT_PORT = <Your_Redshift_Cluster_Port> REDSHIFT_ENDPOINT = \"<Your_Redshift_EndPoint>\" if not schema_name: REDSHIFT_SCHEMA = 'public' else: REDSHIFT_SCHEMA = schema_name REDSHIFT_TABLE = table_name IAM_ROLE = \"arn:aws:iam::<aws_account_number>:role/<Your_Redshift_IAM_Role>\" # REDSHIFT_QUERY = \"SELECT DISTINCT tablename FROM pg_table_def WHERE schemaname = 'public' ORDER BY tablename\" REDSHIFT_COPY_STATEMENT = \"\"\"copy {}.{} FROM '{}' iam_role '{}' TIMEFORMAT AS 'MM/DD/YY HH:MI' ; \"\"\".format(REDSHIFT_SCHEMA, REDSHIFT_TABLE, s3_file, IAM_ROLE) try: conn = pg8000.connect( database=REDSHIFT_DATABASE, user=REDSHIFT_USER, password=REDSHIFT_PASSWD, host=REDSHIFT_ENDPOINT, port=REDSHIFT_PORT ) except Exception as ERROR: print(\"Connection Issue: \" + str(ERROR)) sys.exit(1) try: cursor = conn.cursor() cursor.execute(REDSHIFT_COPY_STATEMENT) cursor.close() conn.commit() conn.close() except Exception as ERROR: print(\"Execution Issue: \" + str(ERROR)) sys.exit(1)","title":"Create AWS Glue Job"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#create-aws-lambda-function-to-invoke-glue-job","text":"Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-trigger-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you created in Create IAM Role for AWS Lambda section Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button Expand Designer section in the same window (section above to Function code ) Click Add trigger button Select S3 in Trigger configuration Select the S3 Bucket you created in Setup S3 bucket and hierarchy section Click Add from datetime import datetime, timedelta import json import boto3 client = boto3.client('glue') dd = boto3.resource('dynamodb') table = dd.Table('DD_JobExecLog') def lambda_handler(event, context): start_timestamp = str(datetime.now()) for record in event['Records']: # Getting S3 Bucket Name from event record bucket = record['s3']['bucket']['name'] # Getting S3 Key from event record key = record['s3']['object']['key'] # Generating complete S3 file path to pass to Glue Job fullS3Path = \"s3://\" + bucket + \"/\" + key # Splitting S3 Key into Schema Name, Table Name and File Name tmp_key = key.split('/',2) schema_name = tmp_key[0] table_name = tmp_key[1] filename = tmp_key[2] glue_job_name = \"glu_techsboot_rsloader\" fullS3Message = \"This Lambda is triggered by - s3://\" + bucket + \"/\" + key # Triggering Glue Job print (\"Triggering Job = \" + glue_job_name) response = client.start_job_run( JobName = glue_job_name, Arguments = { '--schema_name': schema_name, '--table_name': table_name, '--s3_file': fullS3Path } ) # Converting \"response\" from Type dict to string string_response = json.dumps(response) # Parsing JSON response from Glue API parsed_response = json.loads(string_response) ########################### # # table.put_item( Item={ 'JobName': glue_job_name, 'JobRunID': parsed_response['JobRunId'], 'job_state': 'STARTED (Lambda)', 'start_timestamp': start_timestamp, 'update_timestamp': 'null', 'job_message': 'Job Triggered by Lambda', 'job_severity': 'null', 's3_file_key': fullS3Path, 'job_region': 'null', 'job_time': 'null', 'job_account': 'null', 'glue_metadata': parsed_response } ) # Printing Job Metadata in Cloudwatch Log print(\" JOB Metadata \") print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(\" --> JobRunID = \" + parsed_response['JobRunId']) print(\" --> RequestID = \" + parsed_response['ResponseMetadata']['RequestId']) print(\" --> HTTPStatusCode = \" + str(parsed_response['ResponseMetadata']['HTTPStatusCode'])) print(\" --> Timestamp GMT = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['date']) print(\" --> content-type = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-type']) print(\" --> content-length = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['content-length']) print(\" --> connection = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['connection']) print(\" --> x-amzn-requestid = \" + parsed_response['ResponseMetadata']['HTTPHeaders']['x-amzn-requestid']) print(\" --> RetryAttempts = \" + str(parsed_response['ResponseMetadata']['RetryAttempts'])) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\") print(response) print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")","title":"Create AWS Lambda Function to invoke Glue Job"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#create-aws-lambda-function-for-glue-job-progress-and-status-logging","text":"Go to https://console.aws.amazon.com/lambda Click Create function Select Author from scratch Enter techsboot-rsloader-gluelogger-lambda in Function name Select Python 3.7 in Runtime In Permissions section, select Use an existing role and then select the role you create in Create IAM Role for AWS Lambda section Click Create function button at the bottom Copy and Paste following lambda python code in code editor Click Save button from datetime import datetime, timedelta import json import boto3 dd = boto3.resource('dynamodb') table = dd.Table('DD_JobExecLog') def lambda_handler(event, context): #print(\"Received event: \" + json.dumps(event, indent=2)) jobName = event['detail']['jobName'] jobRunId = event['detail']['jobRunId'] job_time = event['time'] j_account = event['account'] j_region = event['region'] j_severity = event['detail']['severity'] j_state = event['detail']['state'] j_message = event['detail']['message'] update_timestamp = str(datetime.now()) # if jobName == \"glu_techsboot_rsloader\": table.update_item( Key={ 'JobName': jobName, 'JobRunID': jobRunId }, UpdateExpression='SET job_message= :msg, job_severity= :sev, update_timestamp = :upd_ts, job_time= :jb_tm, job_region= :j_region, job_state= :v_state, job_account= :acc ', ExpressionAttributeValues={ ':upd_ts': update_timestamp, ':jb_tm': job_time, ':j_region': j_region, ':sev': j_severity, ':v_state': j_state, ':msg': j_message, ':acc': j_account } ) # print(\"current_timestamp: \" + str(datetime.now())) print(\"account: \" + j_account) print(\"time: \" + job_time) print(\"region: \" + j_region) print(\"jobName: \" + jobName) print(\"severity: \" + j_severity) print(\"state: \" + j_state) print(\"jobRunId: \" + jobRunId) print(\"message: \" + j_message) print(\"-----------------------------\") print(\"From Glue Job Event : \" + json.dumps(event))","title":"Create AWS Lambda Function for Glue job progress and status logging"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#add-cloudwatch-event-rule-to-capture-glue-job-status","text":"Go to https://console.aws.amazon.com/cloudwatch Click Rules in left panel under Events section and click Create rule button In Event Srouce section, click on Event Pattern radio button. From Event Pattern dropdown, select Build custom event pattern and copy/paste follwoing JSON { \"source\": [\"aws.glue\"], \"detail-type\": [ \"Glue Job State Change\", \"Glue Job Run Status\" ] } In Targets ( right side ) section, click Add target Select Lambda function in first dropdown Select techsboot-rsloader-gluelogger-lambda lambda fuction created in previous steps in Function dropdown. Click Add target again to add another target Select SNS Topic in first dropdown Select techsboot-rsloader-notification SNS topic ( created in previous steps ) in Topic dropdown. Click Configure details button at the bottom In Name text box, enter techsboot-rsloader-gluestatus-rule and provide a Description text. Click Create rule button","title":"Add CloudWatch Event Rule to capture Glue job status"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#data-loading-and-validation","text":"We are using the following steps for Data loading and validation same as we performed for our Beginner post Pre-load testing Upload sample data into S3 Verify data in Amazon Redshift","title":"Data loading and validation"},{"location":"blogs/aws/rssrvlessetl/intsrvlsglueredshift/#validate-loggin-in-dynamodb-table","text":"Go to https://console.aws.amazon.com/dynamodb Click on Tables in left panel and click on DD_JobExecLog to open table. Click on Items tab to see the job status records in DynamoDB table","title":"Validate loggin in DynamoDB Table"},{"location":"blogs/redshift/RedshiftQMRActvtMon/","text":"Amazon Redshift WLM Query Monitoring Rule (QMR) Action Notification Utility Goals This utility uses a scheduled Lambda function to pull records from the QMR action system log table ( stl_wlm_rule_action ) and publish them to an SNS topic. This utility can be used to send periodic notifications based on the WLM query monitoring rule actions taken for your unique workload and rules configuration. In Amazon Redshift workload management (WLM), query monitoring rules define metrics-based performance boundaries for WLM queues and specify what action to take when a query goes beyond those boundaries. For example, for a queue dedicated to short running queries, you might create a rule that aborts queries that run for more than 60 seconds. To track poorly designed queries, you might have another rule that logs queries that contain nested loops. The rule actions are captured in stl_wlm_rule_action system table. For more information about Redshift workload management (WLM) query monitoring rules and how to configure it, please refer to Redshift Documentation The utility periodically scans stl_wlm_rule_action.actions (log/hop/abort) recorded by WLM query monitoring rules and sends the records as SNS notifications. In summary, a Lambda function is invoked on a scheduled interval, connects to your Redshift cluster, reads events from stl_wlm_rule_action and publishes them to an SNS topic as a JSON string. Installation Notes: Prerequisites This utility requires the following items: VPC: A VPC which currently contains your Amazon Redshift resource and will contain this utility\u2019s Lambda function. NOTE: VPC ID Private Subnets with NAT route: At least two private subnets within that VPC with private routes to the target Amazon Redshift cluster. You should have a NAT Gateway to give access to the Internet for those subnets\u2019 routing tables. You cannot use public subnets. You can read more information on this Lambda requirement here: AWS blog . NOTE: Subnet IDs Security Group: A VPC security group which allows the Lambda function access to your Amazon Redshift cluster on the port specified for SQL connections. NOTE: VPC Security Group ID An Amazon Redshift cluster in the above VPC. NOTE: Amazon Redshift cluster\u2019s Endpoint, Port, Database Database user credentials for an Amazon Redshift user with access to STL_WLM_RULE_ACTION . A superuser will be able to see all rows in this table, and a non-privileged user will be able to see only their own rows. More on visibility here: Visibility of Data in System Tables and Views . NOTE: Amazon Redshift cluster\u2019s user name and password An active WLM configuration with QMR enabled ( Documentation ). Access to an IAM user with privileges to create and modify the necessary CloudFormation, KMS, IAM, SNS, and CloudWatch Events resources. A locally cloned amazon-redshift-utils project containing this utility and AWS CLI and/or AWS Console access. Installation from CloudFormation Template: The quickest way to get up and running with the QMRNotificationUtility is by leveraging the packaged CloudFormation template and the AWS CLI. 1. Navigate to the QMRNotificationUtility\u2019s directory within the amazon-redshift-utils project: git clone/pull amazon-redshift-utils cd amazon-redshift-utils/src/QMRNotificationUtility 2. Copy the zipped python Deployment Package for the Lambda function to a location of your choosing in S3: aws s3 cp ./lambda/dist/qmr-action-notification-utility-1.4.zip s3://yourbucket/qmr-action-notification-utility-1.4.zip 3. Gather the necessary identifiers noted in the prerequistes section above: VPC ID Subnet ID(s) Security Group ID(s) Cluster Endpoint Cluster Port Cluster Database Cluster Credentials (Username and Password) Bucket to host the Lambda Deployment Package Email address to be notified of WLM actions 4. Create the Lambda Function Use the AWS CLI to create a stack containing the necessary dependencies and Lambda function: aws cloudformation create-stack \\ --stack-name qmr-action-notification-utility \\ --template-body file://./cloudformation/qmr-action-notification-utility.yaml \\ --parameters \\ ParameterKey=S3Bucket,ParameterValue=yourbucket \\ ParameterKey=S3Key,ParameterValue=qmr-action-notification-utility-1.4.zip \\ ParameterKey=SNSEmailParameter,ParameterValue=test@email.com \\ ParameterKey=VPC,ParameterValue=vpc-abcd1234 \\ ParameterKey=SubnetIds,ParameterValue=subnet-abcd1234 \\ ParameterKey=SecurityGroupIds,ParameterValue=sg-abcd1234 \\ ParameterKey=RedshiftMonitoringUser,ParameterValue=monitoring_user \\ ParameterKey=RedshiftClusterPort,ParameterValue=cluster_port \\ ParameterKey=RedshiftClusterEndpoint,ParameterValue=examplecluster.abcd12340987.us-east-1.redshift.amazonaws.com \\ ParameterKey=RedshiftClusterDatabase,ParameterValue=db_name \\ ParameterKey=MonitoringDBPasswordCiphertext,ParameterValue= \\ --capabilities CAPABILITY_IAM 5. Verify creation is complete It may take a few mintues for the stack\u2019s resources to be provisioned, and is completed when the following command returns \u201cCREATE_COMPLETE\u201d: aws cloudformation describe-stacks --stack-name qmr-action-notification-utility --query 'Stacks[0].StackStatus' --output text 6. Add an encrypted password From the completed stack creation, extract the KMS Key ID, and use that Key to process your plaintext database password to ciphertext: # Extract KMS Key ID KMSKEYID=`aws cloudformation describe-stack-resource --stack-name qmr-action-notification-utility --logical-resource-id RedshiftKMSKey --query 'StackResourceDetail.PhysicalResourceId' --output text` # Generate a read restricted local file to store your plaintext password (umask 077; touch passwd.txt) # Insert your plaintext password into file. If using vi ensure binary mode and no automatic EOL vi -b -c 'set noeol' passwd.txt # Read plaintext password file contents into kms encrypt to generate ciphertext CIPHERTEXT=`aws kms encrypt --key-id $KMSKEYID --plaintext file://./passwd.txt --query 'CiphertextBlob' --output text` # Cleanup password file rm passwd.txt 7. Update your CloudFormation stack Add the MonitoringDBPasswordCiphertext parameter with the ciphertext generated from the previous step, leaving all other parameters unchanged: aws cloudformation update-stack \\ --stack-name qmr-action-notification-utility \\ --use-previous-template \\ --parameters \\ ParameterKey=MonitoringDBPasswordCiphertext,ParameterValue=$CIPHERTEXT \\ ParameterKey=S3Bucket,UsePreviousValue=true \\ ParameterKey=S3Key,UsePreviousValue=true \\ ParameterKey=SNSEmailParameter,UsePreviousValue=true \\ ParameterKey=VPC,UsePreviousValue=true \\ ParameterKey=SubnetIds,UsePreviousValue=true \\ ParameterKey=SecurityGroupIds,UsePreviousValue=true \\ ParameterKey=RedshiftMonitoringUser,UsePreviousValue=true \\ ParameterKey=RedshiftClusterPort,UsePreviousValue=true \\ ParameterKey=RedshiftClusterEndpoint,UsePreviousValue=true \\ ParameterKey=RedshiftClusterDatabase,UsePreviousValue=true \\ --capabilities CAPABILITY_IAM 8. Verify the modification is complete It may take a moment for the stack\u2019s resources to be updated, and is done when the following command returns \u201cUPDATE_COMPLETE\u201d: aws cloudformation describe-stacks --stack-name qmr-action-notification-utility --query 'Stacks[0].StackStatus' --output text 9. Check the inbox of the email address you included for SNSEmailParameter. There should be an \u201cAWS Notification - Subscription Confirmation\u201d from no-reply@sns.amazonaws.com asking that you confirm your subscription. Click the link if you wish to receive updates on this email address. 10. Verify the email address receives an email notification within 5 minutes By purposely triggering a QMR action by manually running SQL that is known to violate a rule defined in your active WLM configuration. Below is one example SNS notification email message: [ { \"clusterid\":\"examplecluster\", \"database\":\"dev\", \"userid\":100, \"query\":1186777, \"service_class\":6, \"rule\":\"Rule1_Nested_Loop\", \"action\":\"abort\", \"recordtime\":\"2017-06-12T06:51:57.167052\" }, { \"clusterid\":\"examplecluster\", \"database\":\"dev\", \"userid\":100, \"query\":1186999, \"service_class\":6, \"rule\":\"Rule2_high_Return_Rows\", \"action\":\"log\", \"recordtime\":\"2017-06-12T06:53:48.935375\" } ] Rebuilding Lambda Function If you wish to rebuild the Lambda function yourself, you can use lambda/build.sh to create a zipped Deployment Package to upload to your S3 bucket. This utility requires pip and virtualenv python dependencies. This script will initialize a transient virtual environment, download python dependencies from requirements.txt , and zip the lambda function source code with dependencies into a versioned archive for uploading to S3.","title":"Redshift QMR Notification Utility"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#amazon-redshift-wlm-query-monitoring-rule-qmr-action-notification-utility","text":"","title":"Amazon Redshift WLM Query Monitoring Rule (QMR) Action Notification Utility"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#goals","text":"This utility uses a scheduled Lambda function to pull records from the QMR action system log table ( stl_wlm_rule_action ) and publish them to an SNS topic. This utility can be used to send periodic notifications based on the WLM query monitoring rule actions taken for your unique workload and rules configuration. In Amazon Redshift workload management (WLM), query monitoring rules define metrics-based performance boundaries for WLM queues and specify what action to take when a query goes beyond those boundaries. For example, for a queue dedicated to short running queries, you might create a rule that aborts queries that run for more than 60 seconds. To track poorly designed queries, you might have another rule that logs queries that contain nested loops. The rule actions are captured in stl_wlm_rule_action system table. For more information about Redshift workload management (WLM) query monitoring rules and how to configure it, please refer to Redshift Documentation The utility periodically scans stl_wlm_rule_action.actions (log/hop/abort) recorded by WLM query monitoring rules and sends the records as SNS notifications. In summary, a Lambda function is invoked on a scheduled interval, connects to your Redshift cluster, reads events from stl_wlm_rule_action and publishes them to an SNS topic as a JSON string.","title":"Goals"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#installation-notes","text":"","title":"Installation Notes:"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#prerequisites","text":"This utility requires the following items: VPC: A VPC which currently contains your Amazon Redshift resource and will contain this utility\u2019s Lambda function. NOTE: VPC ID Private Subnets with NAT route: At least two private subnets within that VPC with private routes to the target Amazon Redshift cluster. You should have a NAT Gateway to give access to the Internet for those subnets\u2019 routing tables. You cannot use public subnets. You can read more information on this Lambda requirement here: AWS blog . NOTE: Subnet IDs Security Group: A VPC security group which allows the Lambda function access to your Amazon Redshift cluster on the port specified for SQL connections. NOTE: VPC Security Group ID An Amazon Redshift cluster in the above VPC. NOTE: Amazon Redshift cluster\u2019s Endpoint, Port, Database Database user credentials for an Amazon Redshift user with access to STL_WLM_RULE_ACTION . A superuser will be able to see all rows in this table, and a non-privileged user will be able to see only their own rows. More on visibility here: Visibility of Data in System Tables and Views . NOTE: Amazon Redshift cluster\u2019s user name and password An active WLM configuration with QMR enabled ( Documentation ). Access to an IAM user with privileges to create and modify the necessary CloudFormation, KMS, IAM, SNS, and CloudWatch Events resources. A locally cloned amazon-redshift-utils project containing this utility and AWS CLI and/or AWS Console access.","title":"Prerequisites"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#installation-from-cloudformation-template","text":"The quickest way to get up and running with the QMRNotificationUtility is by leveraging the packaged CloudFormation template and the AWS CLI.","title":"Installation from CloudFormation Template:"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#1-navigate-to-the-qmrnotificationutilitys-directory-within-the-amazon-redshift-utils-project","text":"git clone/pull amazon-redshift-utils cd amazon-redshift-utils/src/QMRNotificationUtility","title":"1. Navigate to the QMRNotificationUtility's directory within the amazon-redshift-utils project:"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#2-copy-the-zipped-python-deployment-package-for-the-lambda-function-to-a-location-of-your-choosing-in-s3","text":"aws s3 cp ./lambda/dist/qmr-action-notification-utility-1.4.zip s3://yourbucket/qmr-action-notification-utility-1.4.zip","title":"2. Copy the zipped python Deployment Package for the Lambda function to a location of your choosing in S3:"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#3-gather-the-necessary-identifiers-noted-in-the-prerequistes-section-above","text":"VPC ID Subnet ID(s) Security Group ID(s) Cluster Endpoint Cluster Port Cluster Database Cluster Credentials (Username and Password) Bucket to host the Lambda Deployment Package Email address to be notified of WLM actions","title":"3. Gather the necessary identifiers noted in the prerequistes section above:"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#4-create-the-lambda-function","text":"Use the AWS CLI to create a stack containing the necessary dependencies and Lambda function: aws cloudformation create-stack \\ --stack-name qmr-action-notification-utility \\ --template-body file://./cloudformation/qmr-action-notification-utility.yaml \\ --parameters \\ ParameterKey=S3Bucket,ParameterValue=yourbucket \\ ParameterKey=S3Key,ParameterValue=qmr-action-notification-utility-1.4.zip \\ ParameterKey=SNSEmailParameter,ParameterValue=test@email.com \\ ParameterKey=VPC,ParameterValue=vpc-abcd1234 \\ ParameterKey=SubnetIds,ParameterValue=subnet-abcd1234 \\ ParameterKey=SecurityGroupIds,ParameterValue=sg-abcd1234 \\ ParameterKey=RedshiftMonitoringUser,ParameterValue=monitoring_user \\ ParameterKey=RedshiftClusterPort,ParameterValue=cluster_port \\ ParameterKey=RedshiftClusterEndpoint,ParameterValue=examplecluster.abcd12340987.us-east-1.redshift.amazonaws.com \\ ParameterKey=RedshiftClusterDatabase,ParameterValue=db_name \\ ParameterKey=MonitoringDBPasswordCiphertext,ParameterValue= \\ --capabilities CAPABILITY_IAM","title":"4. Create the Lambda Function"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#5-verify-creation-is-complete","text":"It may take a few mintues for the stack\u2019s resources to be provisioned, and is completed when the following command returns \u201cCREATE_COMPLETE\u201d: aws cloudformation describe-stacks --stack-name qmr-action-notification-utility --query 'Stacks[0].StackStatus' --output text","title":"5. Verify creation is complete"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#6-add-an-encrypted-password","text":"From the completed stack creation, extract the KMS Key ID, and use that Key to process your plaintext database password to ciphertext: # Extract KMS Key ID KMSKEYID=`aws cloudformation describe-stack-resource --stack-name qmr-action-notification-utility --logical-resource-id RedshiftKMSKey --query 'StackResourceDetail.PhysicalResourceId' --output text` # Generate a read restricted local file to store your plaintext password (umask 077; touch passwd.txt) # Insert your plaintext password into file. If using vi ensure binary mode and no automatic EOL vi -b -c 'set noeol' passwd.txt # Read plaintext password file contents into kms encrypt to generate ciphertext CIPHERTEXT=`aws kms encrypt --key-id $KMSKEYID --plaintext file://./passwd.txt --query 'CiphertextBlob' --output text` # Cleanup password file rm passwd.txt","title":"6. Add an encrypted password"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#7-update-your-cloudformation-stack","text":"Add the MonitoringDBPasswordCiphertext parameter with the ciphertext generated from the previous step, leaving all other parameters unchanged: aws cloudformation update-stack \\ --stack-name qmr-action-notification-utility \\ --use-previous-template \\ --parameters \\ ParameterKey=MonitoringDBPasswordCiphertext,ParameterValue=$CIPHERTEXT \\ ParameterKey=S3Bucket,UsePreviousValue=true \\ ParameterKey=S3Key,UsePreviousValue=true \\ ParameterKey=SNSEmailParameter,UsePreviousValue=true \\ ParameterKey=VPC,UsePreviousValue=true \\ ParameterKey=SubnetIds,UsePreviousValue=true \\ ParameterKey=SecurityGroupIds,UsePreviousValue=true \\ ParameterKey=RedshiftMonitoringUser,UsePreviousValue=true \\ ParameterKey=RedshiftClusterPort,UsePreviousValue=true \\ ParameterKey=RedshiftClusterEndpoint,UsePreviousValue=true \\ ParameterKey=RedshiftClusterDatabase,UsePreviousValue=true \\ --capabilities CAPABILITY_IAM","title":"7. Update your CloudFormation stack"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#8-verify-the-modification-is-complete","text":"It may take a moment for the stack\u2019s resources to be updated, and is done when the following command returns \u201cUPDATE_COMPLETE\u201d: aws cloudformation describe-stacks --stack-name qmr-action-notification-utility --query 'Stacks[0].StackStatus' --output text","title":"8. Verify the modification is complete"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#9-check-the-inbox-of-the-email-address-you-included-for-snsemailparameter","text":"There should be an \u201cAWS Notification - Subscription Confirmation\u201d from no-reply@sns.amazonaws.com asking that you confirm your subscription. Click the link if you wish to receive updates on this email address.","title":"9. Check the inbox of the email address you included for SNSEmailParameter."},{"location":"blogs/redshift/RedshiftQMRActvtMon/#10-verify-the-email-address-receives-an-email-notification-within-5-minutes","text":"By purposely triggering a QMR action by manually running SQL that is known to violate a rule defined in your active WLM configuration. Below is one example SNS notification email message: [ { \"clusterid\":\"examplecluster\", \"database\":\"dev\", \"userid\":100, \"query\":1186777, \"service_class\":6, \"rule\":\"Rule1_Nested_Loop\", \"action\":\"abort\", \"recordtime\":\"2017-06-12T06:51:57.167052\" }, { \"clusterid\":\"examplecluster\", \"database\":\"dev\", \"userid\":100, \"query\":1186999, \"service_class\":6, \"rule\":\"Rule2_high_Return_Rows\", \"action\":\"log\", \"recordtime\":\"2017-06-12T06:53:48.935375\" } ]","title":"10. Verify the email address receives an email notification within 5 minutes"},{"location":"blogs/redshift/RedshiftQMRActvtMon/#rebuilding-lambda-function","text":"If you wish to rebuild the Lambda function yourself, you can use lambda/build.sh to create a zipped Deployment Package to upload to your S3 bucket. This utility requires pip and virtualenv python dependencies. This script will initialize a transient virtual environment, download python dependencies from requirements.txt , and zip the lambda function source code with dependencies into a versioned archive for uploading to S3.","title":"Rebuilding Lambda Function"},{"location":"blogs/redshift/Vacclyze/","text":"Disclaimer: var git_user = \"awslabs\"; var git_tree = \"\" var git_repo_name = \"AnalyzeVacuumUtility\"; var git_repo_path = git_user + \"/amazon-redshift-utils/tree/master/src/AnalyzeVacuumUtility\"; // // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='https://\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk and <b>should not be considered as Official sources by any mean through this website.<b></i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line; Analyze & Vacuum Schema Utility In order to get the best performance from your Redshift Database, you must ensure that database tables regularly analyzed and vacuumed. For more information , please read the below Redshift documentation, Vacuuming Tables Analyzing Tables Whenever you insert, delete, or update (In Redshift update = delete + insert) a significant number of rows, you should run a VACUUM command and then an ANALYZE command. In Redshift, the data blocks are immutable, i.e. when rows are DELETED or UPDATED against a table they are simply logically deleted (flagged for deletion), but not physically removed from disk. This causes the rows to continue consuming disk space and those blocks are scanned when a query scans the table. The result of this, table storage space is increased and degraded performance due to otherwise avoidable disk IO during scans. A vacuum recovers the space from deleted rows and restores the sort order. To avoid resource intensive VACUUM operation, you can load the data in sort key order, or design your table maintain data for a rolling time period, using time series tables. If your table has a large unsorted region (which can\u2019t be vacuumed), a deep copy is much faster than a vacuum. You can use the Column Encoding Utility from our open source GitHub project https://github.com/awslabs/amazon-redshift-utils to perform a deep copy. The Column Encoding Utility takes care of the compression analysis, column encoding and deep copy. The ANALYZE command updates the statistics metadata, which enables the query optimizer to generate more accurate query plans. COPY automatically updates statistics after loading an empty table, so your statistics should be up to date. The Redshift \u2018Analyze Vacuum Utility\u2019 gives you the ability to automate VACUUM and ANALYZE operations. When run, it will VACUUM or ANALYZE an entire schema or individual tables. This Utility Analyzes and Vacuums table(s) in a Redshift Database schema, based on certain parameters like unsorted, stats off and size of the table and system alerts from stl_explain & stl_alert_event_log . By turning on/off \u2018\u2013analyze-flag\u2019 and \u2018\u2013vacuum-flag\u2019 parameters, you can run it as \u2018vacuum-only\u2019 or \u2018analyze-only\u2019 utility. This script can be scheduled to run VACUUM and ANALYZE as part of regular maintenance/housekeeping activities, when there are fewer database activities. Vacuum This script runs vacuum in two phases, Phase 1: Identify and run vacuum based on the alerts recorded in stl_alert_event_log . stl_alert_event_log , records an alert when the query optimizer identifies conditions that might indicate performance issues. We can use the stl_alert_event_log table to identify the top 25 tables that need vacuum. The script uses SQL to get the list of tables and number of alerts, which indicate that vacuum is required. Variables affecting this phase: goback_no_of_days : To control number days to look back from CURRENT_DATE Phase 2: Identify and run vacuum based on certain thresholds related to table statistics (Like unsorted > 10% and Stats Off > 10% and limited to specific table sizes. Variables affecting this phase: stats_off_pct : To control the threshold of statistics inaccuracy min_unsorted_pct : To control the lower limit of unsorted blocks max_unsorted_pct : To control the upper limit of unsorted blocks (preventing vacuum on super large tables) max_tbl_size_mb : To control when a table is too large to be vacuumed by this utility Analyze This script runs Analyze in two phases: Phase 1: Run ANALYZE based on the alerts recorded in stl_explain & stl_alert_event_log . Phase 2: Run ANALYZE based the stats_off metric in svv_table_info . If table has a stats_off_pct > 10%, then the script runs ANALYZE command to update the statistics. Summary of Parameters: Parameter Mandatory Default Value \u2013db Yes \u2013db-user Yes \u2013db-pwd Yes \u2013db-host Yes \u2013db-port No 5439 \u2013db-conn-opts No \u2013schema-name No Public \u2013table-name No \u2013output-file Yes \u2013debug No False \u2013slot-count No 1 \u2013ignore-errors No False \u2013query_group No None \u2013analyze-flag No False \u2013vacuum-flag No False \u2013vacuum-parameter No FULL \u2013min-unsorted-pct No 0.05 \u2013max-unsorted-pct No 0.5 \u2013stats-off-pct No 0.1 \u2013stats-threshold No 0.1 \u2013max-table-size-mb No 700*1024 \u2013predicate-cols No False The above parameter values depend on the cluster type, table size, available system resources and available \u2018Time window\u2019 etc. The default values provided here are based on ds2.8xlarge, 8 node cluster. It may take some trial and error to come up with correct parameter values to vacuum and analyze your table(s). If table size is greater than certain size ( max_table_size_mb ) and has a large unsorted region ( max_unsorted_pct ), consider performing a deep copy, which will be much faster than a vacuum. As VACUUM & ANALYZE operations are resource intensive, you should ensure that this will not adversely impact other database operations running on your cluster. AWS has thoroughly tested this software on a variety of systems, but cannot be responsible for the impact of running the utility against your database. Parameter Guidance Schema Name The utility will accept a valid schema name, or alternative a regular expression pattern which will be used to match to all schemas in the database. This uses Posix regular expression syntax. You can use (.*) to match all schemas. Slot Count Sets the number of query slots a query will use. Workload management (WLM) reserves slots in a service class according to the concurrency level set for the queue (for example, if concurrency level is set to 5, then the service class has 5 slots). WLM allocates the available memory for a service class equally to each slot. For more information, see Implementing Workload Management. For operations where performance is heavily affected by the amount of memory allocated, such as Vacuum, increasing the value of wlm_query_slot_count can improve performance. In particular, for slow Vacuum commands, inspect the corresponding record in the SVV_VACUUM_SUMMARY view. If you see high values (close to or higher than 100) for sort_partitions and merge_increments in the SVV_VACUUM_SUMMARY view, consider increasing the value for wlm_query_slot_count the next time you run Vacuum against that table. Increasing the value of wlm_query_slot_count limits the number of concurrent queries that can be run. Note: If the value of wlm_query_slot_count is larger than the number of available slots (concurrency level) for the queue targeted by the user, the utilty will fail. If you encounter an error, decrease wlm_query_slot_count to an allowable value. analyze-flag Flag to turn ON/OFF ANALYZE functionality (True or False). If you want run the script to only perform ANALYZE on a schema or table, set this value \u2018False\u2019 : Default = \u2018False\u2019. vacuum-flag Flag to turn ON/OFF VACUUM functionality (True or False). If you want run the script to only perform VACUUM on a schema or table, set this value \u2018False\u2019 : Default = \u2018False\u2019. vacuum-parameter Specify vacuum parameters [ FULL | SORT ONLY | DELETE ONLY | REINDEX ] Default = FULL min-unsorted-pct Minimum unsorted percentage (%) to consider a table for vacuum: Default = 5%. max-unsorted-pct Maximum unsorted percentage(%) to consider a table for vacuum : Default = 50%. stats-off-pct Minimum stats off percentage(%) to consider a table for analyze : Default = 10% max-table-size-mb Maximum table size 700GB in MB : Default = 700*1024 MB predicate-cols Analyze predicate columns only. Default = False Sample Usage python analyze-vacuum-schema.py --db <> --db-user <> --db-pwd <> --db-port 8192 --db-host aaa.us-west-2.redshift.amazonaws.com --schema-name public --table-name customer_v6 --output-file /Users/test.log --debug True --ignore-errors False --slot-count 2 --min-unsorted-pct 5 --max-unsorted-pct 50 --stats-off-pct 10 --max-table-size-mb 700*1024 Install Notes sudo easy_install pip sudo pip install Limitations Script runs all VACUUM commands sequentially. Currently in Redshift multiple concurrent vacuum operations are not supported. Script runs all ANALYZE commands sequentially not concurrently. Does not support column level ANALYZE. Multiple schemas are not supported. Skew factor is not considered.","title":"Redshift Vacuum Analyze"},{"location":"blogs/redshift/Vacclyze/#analyze-vacuum-schema-utility","text":"In order to get the best performance from your Redshift Database, you must ensure that database tables regularly analyzed and vacuumed. For more information , please read the below Redshift documentation, Vacuuming Tables Analyzing Tables Whenever you insert, delete, or update (In Redshift update = delete + insert) a significant number of rows, you should run a VACUUM command and then an ANALYZE command. In Redshift, the data blocks are immutable, i.e. when rows are DELETED or UPDATED against a table they are simply logically deleted (flagged for deletion), but not physically removed from disk. This causes the rows to continue consuming disk space and those blocks are scanned when a query scans the table. The result of this, table storage space is increased and degraded performance due to otherwise avoidable disk IO during scans. A vacuum recovers the space from deleted rows and restores the sort order. To avoid resource intensive VACUUM operation, you can load the data in sort key order, or design your table maintain data for a rolling time period, using time series tables. If your table has a large unsorted region (which can\u2019t be vacuumed), a deep copy is much faster than a vacuum. You can use the Column Encoding Utility from our open source GitHub project https://github.com/awslabs/amazon-redshift-utils to perform a deep copy. The Column Encoding Utility takes care of the compression analysis, column encoding and deep copy. The ANALYZE command updates the statistics metadata, which enables the query optimizer to generate more accurate query plans. COPY automatically updates statistics after loading an empty table, so your statistics should be up to date. The Redshift \u2018Analyze Vacuum Utility\u2019 gives you the ability to automate VACUUM and ANALYZE operations. When run, it will VACUUM or ANALYZE an entire schema or individual tables. This Utility Analyzes and Vacuums table(s) in a Redshift Database schema, based on certain parameters like unsorted, stats off and size of the table and system alerts from stl_explain & stl_alert_event_log . By turning on/off \u2018\u2013analyze-flag\u2019 and \u2018\u2013vacuum-flag\u2019 parameters, you can run it as \u2018vacuum-only\u2019 or \u2018analyze-only\u2019 utility. This script can be scheduled to run VACUUM and ANALYZE as part of regular maintenance/housekeeping activities, when there are fewer database activities.","title":"Analyze &amp; Vacuum Schema Utility"},{"location":"blogs/redshift/Vacclyze/#vacuum","text":"This script runs vacuum in two phases,","title":"Vacuum"},{"location":"blogs/redshift/Vacclyze/#phase-1","text":"Identify and run vacuum based on the alerts recorded in stl_alert_event_log . stl_alert_event_log , records an alert when the query optimizer identifies conditions that might indicate performance issues. We can use the stl_alert_event_log table to identify the top 25 tables that need vacuum. The script uses SQL to get the list of tables and number of alerts, which indicate that vacuum is required. Variables affecting this phase: goback_no_of_days : To control number days to look back from CURRENT_DATE","title":"Phase 1:"},{"location":"blogs/redshift/Vacclyze/#phase-2","text":"Identify and run vacuum based on certain thresholds related to table statistics (Like unsorted > 10% and Stats Off > 10% and limited to specific table sizes. Variables affecting this phase: stats_off_pct : To control the threshold of statistics inaccuracy min_unsorted_pct : To control the lower limit of unsorted blocks max_unsorted_pct : To control the upper limit of unsorted blocks (preventing vacuum on super large tables) max_tbl_size_mb : To control when a table is too large to be vacuumed by this utility","title":"Phase 2:"},{"location":"blogs/redshift/Vacclyze/#analyze","text":"This script runs Analyze in two phases:","title":"Analyze"},{"location":"blogs/redshift/Vacclyze/#phase-1_1","text":"Run ANALYZE based on the alerts recorded in stl_explain & stl_alert_event_log .","title":"Phase 1:"},{"location":"blogs/redshift/Vacclyze/#phase-2_1","text":"Run ANALYZE based the stats_off metric in svv_table_info . If table has a stats_off_pct > 10%, then the script runs ANALYZE command to update the statistics.","title":"Phase 2:"},{"location":"blogs/redshift/Vacclyze/#summary-of-parameters","text":"Parameter Mandatory Default Value \u2013db Yes \u2013db-user Yes \u2013db-pwd Yes \u2013db-host Yes \u2013db-port No 5439 \u2013db-conn-opts No \u2013schema-name No Public \u2013table-name No \u2013output-file Yes \u2013debug No False \u2013slot-count No 1 \u2013ignore-errors No False \u2013query_group No None \u2013analyze-flag No False \u2013vacuum-flag No False \u2013vacuum-parameter No FULL \u2013min-unsorted-pct No 0.05 \u2013max-unsorted-pct No 0.5 \u2013stats-off-pct No 0.1 \u2013stats-threshold No 0.1 \u2013max-table-size-mb No 700*1024 \u2013predicate-cols No False The above parameter values depend on the cluster type, table size, available system resources and available \u2018Time window\u2019 etc. The default values provided here are based on ds2.8xlarge, 8 node cluster. It may take some trial and error to come up with correct parameter values to vacuum and analyze your table(s). If table size is greater than certain size ( max_table_size_mb ) and has a large unsorted region ( max_unsorted_pct ), consider performing a deep copy, which will be much faster than a vacuum. As VACUUM & ANALYZE operations are resource intensive, you should ensure that this will not adversely impact other database operations running on your cluster. AWS has thoroughly tested this software on a variety of systems, but cannot be responsible for the impact of running the utility against your database.","title":"Summary of Parameters:"},{"location":"blogs/redshift/Vacclyze/#parameter-guidance","text":"","title":"Parameter Guidance"},{"location":"blogs/redshift/Vacclyze/#schema-name","text":"The utility will accept a valid schema name, or alternative a regular expression pattern which will be used to match to all schemas in the database. This uses Posix regular expression syntax. You can use (.*) to match all schemas.","title":"Schema Name"},{"location":"blogs/redshift/Vacclyze/#slot-count","text":"Sets the number of query slots a query will use. Workload management (WLM) reserves slots in a service class according to the concurrency level set for the queue (for example, if concurrency level is set to 5, then the service class has 5 slots). WLM allocates the available memory for a service class equally to each slot. For more information, see Implementing Workload Management. For operations where performance is heavily affected by the amount of memory allocated, such as Vacuum, increasing the value of wlm_query_slot_count can improve performance. In particular, for slow Vacuum commands, inspect the corresponding record in the SVV_VACUUM_SUMMARY view. If you see high values (close to or higher than 100) for sort_partitions and merge_increments in the SVV_VACUUM_SUMMARY view, consider increasing the value for wlm_query_slot_count the next time you run Vacuum against that table. Increasing the value of wlm_query_slot_count limits the number of concurrent queries that can be run. Note: If the value of wlm_query_slot_count is larger than the number of available slots (concurrency level) for the queue targeted by the user, the utilty will fail. If you encounter an error, decrease wlm_query_slot_count to an allowable value.","title":"Slot Count"},{"location":"blogs/redshift/Vacclyze/#analyze-flag","text":"Flag to turn ON/OFF ANALYZE functionality (True or False). If you want run the script to only perform ANALYZE on a schema or table, set this value \u2018False\u2019 : Default = \u2018False\u2019.","title":"analyze-flag"},{"location":"blogs/redshift/Vacclyze/#vacuum-flag","text":"Flag to turn ON/OFF VACUUM functionality (True or False). If you want run the script to only perform VACUUM on a schema or table, set this value \u2018False\u2019 : Default = \u2018False\u2019.","title":"vacuum-flag"},{"location":"blogs/redshift/Vacclyze/#vacuum-parameter","text":"Specify vacuum parameters [ FULL | SORT ONLY | DELETE ONLY | REINDEX ] Default = FULL","title":"vacuum-parameter"},{"location":"blogs/redshift/Vacclyze/#min-unsorted-pct","text":"Minimum unsorted percentage (%) to consider a table for vacuum: Default = 5%.","title":"min-unsorted-pct"},{"location":"blogs/redshift/Vacclyze/#max-unsorted-pct","text":"Maximum unsorted percentage(%) to consider a table for vacuum : Default = 50%.","title":"max-unsorted-pct"},{"location":"blogs/redshift/Vacclyze/#stats-off-pct","text":"Minimum stats off percentage(%) to consider a table for analyze : Default = 10%","title":"stats-off-pct"},{"location":"blogs/redshift/Vacclyze/#max-table-size-mb","text":"Maximum table size 700GB in MB : Default = 700*1024 MB","title":"max-table-size-mb"},{"location":"blogs/redshift/Vacclyze/#predicate-cols","text":"Analyze predicate columns only. Default = False","title":"predicate-cols"},{"location":"blogs/redshift/Vacclyze/#sample-usage","text":"python analyze-vacuum-schema.py --db <> --db-user <> --db-pwd <> --db-port 8192 --db-host aaa.us-west-2.redshift.amazonaws.com --schema-name public --table-name customer_v6 --output-file /Users/test.log --debug True --ignore-errors False --slot-count 2 --min-unsorted-pct 5 --max-unsorted-pct 50 --stats-off-pct 10 --max-table-size-mb 700*1024","title":"Sample Usage"},{"location":"blogs/redshift/Vacclyze/#install-notes","text":"sudo easy_install pip sudo pip install","title":"Install Notes"},{"location":"blogs/redshift/Vacclyze/#limitations","text":"Script runs all VACUUM commands sequentially. Currently in Redshift multiple concurrent vacuum operations are not supported. Script runs all ANALYZE commands sequentially not concurrently. Does not support column level ANALYZE. Multiple schemas are not supported. Skew factor is not considered.","title":"Limitations"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/","text":"Disclaimer: var git_user = \"awslabs\"; var git_tree = \"\" var git_repo_name = \"SystemTablePersistence\"; var git_repo_path = git_user + \"/amazon-redshift-utils/tree/master/src/SystemTablePersistence\"; // // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='https://\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk and <b>should not be considered as Official sources by any mean through this website.<b></i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line; Amazon Redshift System Object Persistence Utility Amazon Redshift, like most databases, contains monitoring and diagnostic information in the form of internal tables and views that can be queried to understand system behaviour better. Redshift system tables and views, numbering over 100, have a system-controlled retention that is variable but tends to be less than a week for common Redshift use-cases. This link outlines the most important tables and views for diagnostic performance information. The stl_ prefix denotes system table logs. stl_ tables contain logs about operations that happened on the cluster in the past few days. The stv_ prefix denotes system table snapshots. stv_ tables contain a snapshot of the current state of the cluster. The svl_ prefix denotes system view logs. svl_ views join some number of system tables to provide more descriptive info. The svv_ prefix denotes system view snapshots. Like the svl_ views, the svv_ views join some system tables to provide more descriptive info. To persist the tables for a longer amount of time, this project provides an example implementation to create, populate, and use five of the most common objects that we see requiring long term retention. This mix of tables and views will highlight some of the edge cases users will encounter when applying tuning techniques techniques to their own list of tables. Deploying The Redshift Automation project can be used to host and run this utility, plus others including table analysis and vacuum, and can be setup with a one-click deployment to AWS Lambda. We have provided the following AWS SAM templates so that you can deploy this function as stand-alone, without the other functions from the wider RedshiftAutomation modules (please note that we currently only support deploying into VPC): Region Template ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 We\u2019ve also run across some customers who already have an EC2 host for cron/scheduling related activities. If you wish to use ec2 or other runners with cron, then the Redshift Automation command line provides an option to run this application: ./ra --utility SystemTablePersistence --config s3://mybucket/prefix/config.json Manual Setup Actions (optional if you are using the above Lambda function): Creating the HISTORY Schema: Creating a separate schema is a convenient way to sequester the history tables and views away from other objects. Any preferred schema name can be used; a case-insensitive search-and-replace of \u201cHISTORY\u201d will correctly update all the schema references in the included SQL. CREATE SCHEMA IF NOT EXISTS history; Creating the History Tables: The persisted data will be stored in direct-attached storage tables in Redshift. For the tables, the CREATE TABLE {tablename} LIKE technique is an easy way to inherit the column names, datatypes, encoding, and table distribution from system tables. For system views, an Internet search on the view name (for example: SVL_QUERY_SUMMARY ) will direct the user to the Redshift documentation which includes the column-level description. This table will frequently copy-and-paste well into a spreadsheet program, allow for easy extraction of the column name and data type information. History tables are created in the HISTORY schema, and will be verified on each run of the table persistence system. You can view the table creation statements in history_table_creation.sql . Creating the Views to Join the Historical and Current Information: The views return data from both the current system objects and the historical tables. The anti-join pattern is used to accomplish the deduplication of rows. CREATE OR REPLACE VIEW history.all_stl_load_errors AS ( SELECT le.* FROM stl_load_errors le UNION ALL SELECT h.* FROM stl_load_errors le RIGHT OUTER JOIN history.hist_stl_load_errors h ON (le.query = h.query AND le.starttime = h.starttime) WHERE le.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_query AS ( SELECT q.* FROM stl_query q UNION ALL SELECT h.* FROM stl_query q RIGHT OUTER JOIN history.hist_stl_query h ON (q.query = h.query AND q.starttime = h.starttime) WHERE q.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_wlm_query AS ( SELECT wq.* FROM stl_wlm_query wq UNION ALL SELECT h.* FROM stl_wlm_query wq RIGHT OUTER JOIN history.hist_stl_wlm_query h ON (wq.query = h.query AND wq.service_class_start_time = h.service_class_start_time) WHERE wq.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_explain AS ( SELECT e.* FROM stl_explain e UNION ALL SELECT h.* FROM stl_explain e RIGHT OUTER JOIN history.hist_stl_explain h ON (e.query = h.query AND e.userid = h.userid AND e.nodeid = h.nodeid AND e.parentid = h.parentid AND e.plannode = h.plannode) WHERE e.query IS NULL ); CREATE OR REPLACE VIEW history.all_svl_query_summary AS ( SELECT qs.* FROM svl_query_summary qs UNION ALL SELECT h.* FROM svl_query_summary qs RIGHT OUTER JOIN history.hist_svl_query_summary h ON (qs.query = h.query AND qs.userid = h.userid AND qs.stm = h.stm AND qs.seg = h.seg AND qs.step = h.step AND qs.maxtime = h.maxtime AND qs.label = h.label) WHERE qs.query IS NULL ); Populating the History Tables: This can be done Daily or on a User-Selected Frequency (we recommend populating the history tables daily). This utility will insert only the new rows using the model as described above, with an anti-join: INSERT INTO history.hist_stl_load_errors ( SELECT le.* FROM stl_load_errors le, (SELECT NVL(MAX(starttime),'01/01/1902'::TIMESTAMP) AS max_starttime FROM history.hist_stl_load_errors) h WHERE le.starttime > h.max_starttime); You can view the statements that will be run by the utility in history_table_config.json ; Querying the Views in the History Schema: The history schema views can be queried in exactly the same way that users have interacted with the existing system objects. SELECT * FROM history.all_stl_load_errors WHERE UPPER(err_reason) LIKE '%DELIMITER NOT FOUND%'; SELECT * FROM history.all_stl_query WHERE query = 1121; SELECT COUNT(*) FROM history.all_stl_wlm_query WHERE service_class = 6; SELECT * FROM history.all_stl_explain WHERE query = 1121 ORDER BY nodeid; SELECT * FROM history.all_svl_query_summary WHERE bytes > 1000000; Long term archival of System Table data If required, this utility can archive data from the internal HISTORY tables to Amazon S3. Add the following configuration values to your configuration file: \"s3_unload_location\":\"s3://mybucket/prefix/redshift/systables/archive\" \"s3_unload_role_arn\":\"arn:aws:iam::<acct>:role/<role name>\" Please note that the s3_unload_role_arn should be linked to the Redshift cluster to enable S3 access as outlined here . The structure of the exported data will make it suitable for querying via the AWS Glue Data Catalog. Both the cluster-name and the datetime of data export will be configured as partitions. The structure of the data in the exported location will be: Configured output location <table name> - such as hist_svl_query_summary , hist_stl_explain cluster=<your cluster name> datetime=<exported date time information> For example: All exported data is quoted, compressed csv data. Other Considerations: As with any table in Redshift, it\u2019s a best practice to analyze (even just a handful of columns) on a weekly basis. This will help inform the query planner of the attributes of the table. Users may also want to enhance the performance of the history views by adding sort keys to the underlying history tables. It is recommended to consider columns used in the filter condition on the associated view for good sort key candidates.","title":"System Table Persistence"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#amazon-redshift-system-object-persistence-utility","text":"Amazon Redshift, like most databases, contains monitoring and diagnostic information in the form of internal tables and views that can be queried to understand system behaviour better. Redshift system tables and views, numbering over 100, have a system-controlled retention that is variable but tends to be less than a week for common Redshift use-cases. This link outlines the most important tables and views for diagnostic performance information. The stl_ prefix denotes system table logs. stl_ tables contain logs about operations that happened on the cluster in the past few days. The stv_ prefix denotes system table snapshots. stv_ tables contain a snapshot of the current state of the cluster. The svl_ prefix denotes system view logs. svl_ views join some number of system tables to provide more descriptive info. The svv_ prefix denotes system view snapshots. Like the svl_ views, the svv_ views join some system tables to provide more descriptive info. To persist the tables for a longer amount of time, this project provides an example implementation to create, populate, and use five of the most common objects that we see requiring long term retention. This mix of tables and views will highlight some of the edge cases users will encounter when applying tuning techniques techniques to their own list of tables.","title":"Amazon Redshift System Object Persistence Utility"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#deploying","text":"The Redshift Automation project can be used to host and run this utility, plus others including table analysis and vacuum, and can be setup with a one-click deployment to AWS Lambda. We have provided the following AWS SAM templates so that you can deploy this function as stand-alone, without the other functions from the wider RedshiftAutomation modules (please note that we currently only support deploying into VPC): Region Template ap-northeast-1 ap-northeast-2 ap-south-1 ap-southeast-1 ap-southeast-2 ca-central-1 eu-central-1 eu-west-1 eu-west-2 sa-east-1 us-east-1 us-east-2 us-west-1 us-west-2 We\u2019ve also run across some customers who already have an EC2 host for cron/scheduling related activities. If you wish to use ec2 or other runners with cron, then the Redshift Automation command line provides an option to run this application: ./ra --utility SystemTablePersistence --config s3://mybucket/prefix/config.json","title":"Deploying"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#manual-setup-actions-optional-if-you-are-using-the-above-lambda-function","text":"","title":"Manual Setup Actions (optional if you are using the above Lambda function):"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#creating-the-history-schema","text":"Creating a separate schema is a convenient way to sequester the history tables and views away from other objects. Any preferred schema name can be used; a case-insensitive search-and-replace of \u201cHISTORY\u201d will correctly update all the schema references in the included SQL. CREATE SCHEMA IF NOT EXISTS history;","title":"Creating the HISTORY Schema:"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#creating-the-history-tables","text":"The persisted data will be stored in direct-attached storage tables in Redshift. For the tables, the CREATE TABLE {tablename} LIKE technique is an easy way to inherit the column names, datatypes, encoding, and table distribution from system tables. For system views, an Internet search on the view name (for example: SVL_QUERY_SUMMARY ) will direct the user to the Redshift documentation which includes the column-level description. This table will frequently copy-and-paste well into a spreadsheet program, allow for easy extraction of the column name and data type information. History tables are created in the HISTORY schema, and will be verified on each run of the table persistence system. You can view the table creation statements in history_table_creation.sql .","title":"Creating the History Tables:"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#creating-the-views-to-join-the-historical-and-current-information","text":"The views return data from both the current system objects and the historical tables. The anti-join pattern is used to accomplish the deduplication of rows. CREATE OR REPLACE VIEW history.all_stl_load_errors AS ( SELECT le.* FROM stl_load_errors le UNION ALL SELECT h.* FROM stl_load_errors le RIGHT OUTER JOIN history.hist_stl_load_errors h ON (le.query = h.query AND le.starttime = h.starttime) WHERE le.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_query AS ( SELECT q.* FROM stl_query q UNION ALL SELECT h.* FROM stl_query q RIGHT OUTER JOIN history.hist_stl_query h ON (q.query = h.query AND q.starttime = h.starttime) WHERE q.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_wlm_query AS ( SELECT wq.* FROM stl_wlm_query wq UNION ALL SELECT h.* FROM stl_wlm_query wq RIGHT OUTER JOIN history.hist_stl_wlm_query h ON (wq.query = h.query AND wq.service_class_start_time = h.service_class_start_time) WHERE wq.query IS NULL ); CREATE OR REPLACE VIEW history.all_stl_explain AS ( SELECT e.* FROM stl_explain e UNION ALL SELECT h.* FROM stl_explain e RIGHT OUTER JOIN history.hist_stl_explain h ON (e.query = h.query AND e.userid = h.userid AND e.nodeid = h.nodeid AND e.parentid = h.parentid AND e.plannode = h.plannode) WHERE e.query IS NULL ); CREATE OR REPLACE VIEW history.all_svl_query_summary AS ( SELECT qs.* FROM svl_query_summary qs UNION ALL SELECT h.* FROM svl_query_summary qs RIGHT OUTER JOIN history.hist_svl_query_summary h ON (qs.query = h.query AND qs.userid = h.userid AND qs.stm = h.stm AND qs.seg = h.seg AND qs.step = h.step AND qs.maxtime = h.maxtime AND qs.label = h.label) WHERE qs.query IS NULL );","title":"Creating the Views to Join the Historical and Current Information:"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#populating-the-history-tables","text":"This can be done Daily or on a User-Selected Frequency (we recommend populating the history tables daily). This utility will insert only the new rows using the model as described above, with an anti-join: INSERT INTO history.hist_stl_load_errors ( SELECT le.* FROM stl_load_errors le, (SELECT NVL(MAX(starttime),'01/01/1902'::TIMESTAMP) AS max_starttime FROM history.hist_stl_load_errors) h WHERE le.starttime > h.max_starttime); You can view the statements that will be run by the utility in history_table_config.json ;","title":"Populating the History Tables:"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#querying-the-views-in-the-history-schema","text":"The history schema views can be queried in exactly the same way that users have interacted with the existing system objects. SELECT * FROM history.all_stl_load_errors WHERE UPPER(err_reason) LIKE '%DELIMITER NOT FOUND%'; SELECT * FROM history.all_stl_query WHERE query = 1121; SELECT COUNT(*) FROM history.all_stl_wlm_query WHERE service_class = 6; SELECT * FROM history.all_stl_explain WHERE query = 1121 ORDER BY nodeid; SELECT * FROM history.all_svl_query_summary WHERE bytes > 1000000;","title":"Querying the Views in the History Schema:"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#long-term-archival-of-system-table-data","text":"If required, this utility can archive data from the internal HISTORY tables to Amazon S3. Add the following configuration values to your configuration file: \"s3_unload_location\":\"s3://mybucket/prefix/redshift/systables/archive\" \"s3_unload_role_arn\":\"arn:aws:iam::<acct>:role/<role name>\" Please note that the s3_unload_role_arn should be linked to the Redshift cluster to enable S3 access as outlined here . The structure of the exported data will make it suitable for querying via the AWS Glue Data Catalog. Both the cluster-name and the datetime of data export will be configured as partitions. The structure of the data in the exported location will be: Configured output location <table name> - such as hist_svl_query_summary , hist_stl_explain cluster=<your cluster name> datetime=<exported date time information> For example: All exported data is quoted, compressed csv data.","title":"Long term archival of System Table data"},{"location":"blogs/redshift/awslabs_SystemTablePersistence/#other-considerations","text":"As with any table in Redshift, it\u2019s a best practice to analyze (even just a handful of columns) on a weekly basis. This will help inform the query planner of the attributes of the table. Users may also want to enhance the performance of the history views by adding sort keys to the underlying history tables. It is recommended to consider columns used in the filter condition on the associated view for good sort key candidates.","title":"Other Considerations:"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/","text":"Important This post is to help customers who are looking for AES encryption and decryption UDF implementation in Amazon Redshift to answer \u201cHow AES encryption and decryption UDF can be implemented in Amazon Redshift?\u201d. This post should not be considered as \u201cBest Practice\u201d for such implementation. How to implement AES Encryption & Decryption UDF in Amazon Redshift What is this post about? This post is to help customers who are looking for AES encryption and decryption UDF implementation in Amazon Redshift to answer \"How AES encryption and decryption UDF can be implemented in Amazon Redshift?\". Beside Amazon Redshift cluster level encryption, from data governance and protection perspective often customers want to use Advanced Encryption Standard (AES) for their sensitive data to be stored in Redshift database columns. As data loading is part of ELT, this is often helpful for ELT developers and architects to simplify their ELT process for Amazon Redshift and reducing development efforts using User Defined Function where developers need to encrypt data. At the same time, authorized users and data scientist can also leverage the decrypted data using decryption UDF. Advanced Encryption Standard (AES) The Advanced Encryption Standard (AES) is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001. UDF Deployment These user defined functions use PyPI pyaes module to encrypt and decrypt data using AES encrypt and decrypt methods. AES encryption supports 128 bits (16 bytes), 192 bits (24 bytes) or 256 bits (32 bytes), any key length other than supported will throw error during encryption and/or decryption. See for more details: https://github.com/ricmoo/pyaes Create library CREATE OR REPLACE LIBRARY pyaes LANGUAGE plpythonu FROM 'https://tinyurl.com/redshift-udfs/pyaes.zip?raw=true'; ; Create encrypt function --- Create a separate schema to deploy encryption UDF CREATE SCHEMA udf_enc; --- Create UDF CREATE OR REPLACE FUNCTION udf_enc.aes_encrypt(input VARCHAR(max), vKey VARCHAR(256)) RETURNS VARCHAR STABLE AS $$ import pyaes import binascii if input is None: return None key = vKey # Your Key here aes=pyaes.AESModeOfOperationCTR(key) cipher_txt=aes.encrypt(input) cipher_txt2=binascii.hexlify(cipher_txt) return str(cipher_txt2.decode('utf-8')) $$ LANGUAGE plpythonu ; Create decrypt function --- Create a separate schema to deploy Decryption UDF to control decryption access to only authorized users. CREATE SCHEMA udf_dec; --- Create UDF CREATE OR REPLACE FUNCTION udf_dec.aes_decrypt(encrypted_msg varchar(max), vKey VARCHAR(256)) RETURNS VARCHAR STABLE AS $$ import pyaes import binascii if encrypted_msg is None or len(str(encrypted_msg)) == 0: return None key = vKey # Your decryption key here aes = pyaes.AESModeOfOperationCTR(key) encrypted_msg2=binascii.unhexlify(encrypted_msg) decrypted_msg2 = aes.decrypt(encrypted_msg2) return str(decrypted_msg2.decode('utf-8')) $$ LANGUAGE plpythonu ; Test functionality Setup Test Environment and Data --- Create schema to contain sensitive data CREATE SCHEMA secure_edw_t; --- Create views schema CREATE SCHEMA edw_v; --- Create a sample user CREATE USER edw_v_read password 'B!@nK73x7'; --- Grant USAGE privileges on schema to make object visible to user (this will NOT make data visible to user) GRANT USAGE ON SCHEMA secure_edw_t TO edw_v_read; GRANT USAGE ON SCHEMA edw_v TO edw_v_read; --- Create table CREATE TABLE secure_edw_t.emp_secure ( emp_id int, emp_name VARCHAR(255), emp_phone varchar(255), emp_name_enc VARCHAR(255), emp_phone_enc varchar(255) ); --- Create view to project only encrypted values to the user CREATE VIEW edw_v.emp AS ( SELECT emp_id , emp_name_enc as emp_name , emp_phone_enc as emp_phone FROM secure_edw_t.emp_secure ); GRANT SELECT ON edw_v.emp TO edw_v_read; --- Populate test data in Table INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (1, 'Azhar', '2341232345'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (2, 'Humayun', '4323445676'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (3, 'Rawish', '2221233213'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (4, 'Khayyam', '9808987658'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (5, 'Kawish', '2342342456'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (6, 'Shariq', '6575768475'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (7, 'Qasim', '3213453234'); See GRANT for more details about granting appropriate permission in Amazon Redshift. Encrypting Data In this simple test, we are using different keys for corresponding columns for encryption. emp_name is encrypted using key empnameKey/fhci4=dnv73./xorb3f05 emp_phone is encrypted using key empphoneKey29s0vne03]jv023n=bn34 SQL --- Encrypt data UPDATE secure_edw_t.emp_secure SET emp_name_enc = udf_enc.aes_encrypt(emp_name, LPAD('empnameKey/fhci4=dnv73./xorb3f05', 32, 'z')), emp_phone_enc = udf_enc.aes_encrypt(emp_phone, LPAD('empphoneKey29s0vne03]jv023n=bn34', 32, 'z')) ; SELECT * FROM secure_edw_t.emp_secure; Result Actual data in table will be like below: emp_id emp_name emp_phone emp_name_enc emp_phone_enc 1 Azhar 2341232345 e8a769c636 9345901d200893aeca19 2 Humayun 4323445676 e1a86cc63d3194 9545961f260f94abc91a 3 Rawish 2221233213 fbbc76ce372c 9344961d200892afcf1f 4 Khayyam 9808987658 e2b560de3d2597 984e94142b0396abcb14 5 Kawish 2342342456 e2bc76ce372c 9345901e210f93a9cb1a 6 Shariq 6575768475 fab560d52d35 97439319250d99a9c919 7 Qasim 3213453234 f8bc72ce29 9244951f260e92afcd18 Test decryption and user data visibility Test values decryption Using correct key SQL SELECT emp_id, udf_dec.aes_decrypt(emp_name, LPAD('empnameKey/fhci4=dnv73./xorb3f05', 32, 'z')) emp_name, udf_dec.aes_decrypt(emp_phone, LPAD('empphoneKey29s0vne03]jv023n=bn34', 32, 'z')) emp_phone FROM edw_v.emp; Result emp_id emp_name emp_phone 1 Azhar 2341232345 2 Humayun 4323445676 3 Rawish 2221233213 4 Khayyam 9808987658 5 Kawish 2342342456 6 Shariq 6575768475 7 Qasim 3213453234 Using incorrect key SQL SELECT emp_id, udf_dec.aes_decrypt(emp_name, LPAD('empWRONGKey/fhci4=dnv73./xorb3', 32, 'z')) emp_name, aes_decrypt(emp_phone, LPAD('phoneWRONGKey29s0vne03]jv023n=', 32, 'z')) emp_phone FROM edw_v.emp; Result SQL Error [500310] [XX000]: [Amazon](500310) Invalid operation: UnicodeDecodeError: 'utf8' codec can't decode byte 0xfb in position 0: invalid start byte. Please look at svl_udf_log for more information Details: ----------------------------------------------- error: UnicodeDecodeError: 'utf8' codec can't decode byte 0xfb in position 0: invalid start byte. Please look at svl_udf_log for more information code: 10000 context: UDF query: 3405510 location: udf_client.cpp:369 process: query0_2002_3405510 [pid=43012] -----------------------------------------------; Check for error details if error occurred SELECT * FROM svl_udf_log; See svl_udf_log for more details Test data visibility for user Change session user We are using SET SESSION AUTHORIZATION command to switch user session temporary to \u201cedw_v_read\u201d. If you are not running this test using superuser, then you have to logoff and login using the \u201cedw_v_read\u201d credentials to test. See SET SESSION AUTHORIZATION for more details. SET SESSION AUTHORIZATION 'edw_v_read'; Read data from view SQL SELECT * FROM edw_v.emp; Result emp_id emp_name emp_phone 1 e8a769c636 9345901d200893aeca19 2 e1a86cc63d3194 9545961f260f94abc91a 3 fbbc76ce372c 9344961d200892afcf1f 4 e2b560de3d2597 984e94142b0396abcb14 5 e2bc76ce372c 9345901e210f93a9cb1a 6 fab560d52d35 97439319250d99a9c919 7 f8bc72ce29 9244951f260e92afcd18 Attempt to read data from table SQL SELECT * FROM secure_edw_t.emp_secure; Result SQL Error [500310] [42501]: [Amazon](500310) Invalid operation: permission denied for relation emp_secure;","title":"Redshift Encryption UDF"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#how-to-implement-aes-encryption-decryption-udf-in-amazon-redshift","text":"","title":"How to implement AES Encryption &amp; Decryption UDF in Amazon Redshift"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#what-is-this-post-about","text":"This post is to help customers who are looking for AES encryption and decryption UDF implementation in Amazon Redshift to answer \"How AES encryption and decryption UDF can be implemented in Amazon Redshift?\". Beside Amazon Redshift cluster level encryption, from data governance and protection perspective often customers want to use Advanced Encryption Standard (AES) for their sensitive data to be stored in Redshift database columns. As data loading is part of ELT, this is often helpful for ELT developers and architects to simplify their ELT process for Amazon Redshift and reducing development efforts using User Defined Function where developers need to encrypt data. At the same time, authorized users and data scientist can also leverage the decrypted data using decryption UDF.","title":"What is this post about?"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#advanced-encryption-standard-aes","text":"The Advanced Encryption Standard (AES) is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.","title":"Advanced Encryption Standard (AES)"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#udf-deployment","text":"These user defined functions use PyPI pyaes module to encrypt and decrypt data using AES encrypt and decrypt methods. AES encryption supports 128 bits (16 bytes), 192 bits (24 bytes) or 256 bits (32 bytes), any key length other than supported will throw error during encryption and/or decryption. See for more details: https://github.com/ricmoo/pyaes","title":"UDF Deployment"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#create-library","text":"CREATE OR REPLACE LIBRARY pyaes LANGUAGE plpythonu FROM 'https://tinyurl.com/redshift-udfs/pyaes.zip?raw=true'; ;","title":"Create library"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#create-encrypt-function","text":"--- Create a separate schema to deploy encryption UDF CREATE SCHEMA udf_enc; --- Create UDF CREATE OR REPLACE FUNCTION udf_enc.aes_encrypt(input VARCHAR(max), vKey VARCHAR(256)) RETURNS VARCHAR STABLE AS $$ import pyaes import binascii if input is None: return None key = vKey # Your Key here aes=pyaes.AESModeOfOperationCTR(key) cipher_txt=aes.encrypt(input) cipher_txt2=binascii.hexlify(cipher_txt) return str(cipher_txt2.decode('utf-8')) $$ LANGUAGE plpythonu ;","title":"Create encrypt function"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#create-decrypt-function","text":"--- Create a separate schema to deploy Decryption UDF to control decryption access to only authorized users. CREATE SCHEMA udf_dec; --- Create UDF CREATE OR REPLACE FUNCTION udf_dec.aes_decrypt(encrypted_msg varchar(max), vKey VARCHAR(256)) RETURNS VARCHAR STABLE AS $$ import pyaes import binascii if encrypted_msg is None or len(str(encrypted_msg)) == 0: return None key = vKey # Your decryption key here aes = pyaes.AESModeOfOperationCTR(key) encrypted_msg2=binascii.unhexlify(encrypted_msg) decrypted_msg2 = aes.decrypt(encrypted_msg2) return str(decrypted_msg2.decode('utf-8')) $$ LANGUAGE plpythonu ;","title":"Create decrypt function"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#test-functionality","text":"","title":"Test functionality"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#setup-test-environment-and-data","text":"--- Create schema to contain sensitive data CREATE SCHEMA secure_edw_t; --- Create views schema CREATE SCHEMA edw_v; --- Create a sample user CREATE USER edw_v_read password 'B!@nK73x7'; --- Grant USAGE privileges on schema to make object visible to user (this will NOT make data visible to user) GRANT USAGE ON SCHEMA secure_edw_t TO edw_v_read; GRANT USAGE ON SCHEMA edw_v TO edw_v_read; --- Create table CREATE TABLE secure_edw_t.emp_secure ( emp_id int, emp_name VARCHAR(255), emp_phone varchar(255), emp_name_enc VARCHAR(255), emp_phone_enc varchar(255) ); --- Create view to project only encrypted values to the user CREATE VIEW edw_v.emp AS ( SELECT emp_id , emp_name_enc as emp_name , emp_phone_enc as emp_phone FROM secure_edw_t.emp_secure ); GRANT SELECT ON edw_v.emp TO edw_v_read; --- Populate test data in Table INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (1, 'Azhar', '2341232345'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (2, 'Humayun', '4323445676'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (3, 'Rawish', '2221233213'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (4, 'Khayyam', '9808987658'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (5, 'Kawish', '2342342456'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (6, 'Shariq', '6575768475'); INSERT INTO secure_edw_t.emp_secure (emp_id, emp_name, emp_phone) Values (7, 'Qasim', '3213453234'); See GRANT for more details about granting appropriate permission in Amazon Redshift.","title":"Setup Test Environment and Data"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#encrypting-data","text":"In this simple test, we are using different keys for corresponding columns for encryption. emp_name is encrypted using key empnameKey/fhci4=dnv73./xorb3f05 emp_phone is encrypted using key empphoneKey29s0vne03]jv023n=bn34 SQL --- Encrypt data UPDATE secure_edw_t.emp_secure SET emp_name_enc = udf_enc.aes_encrypt(emp_name, LPAD('empnameKey/fhci4=dnv73./xorb3f05', 32, 'z')), emp_phone_enc = udf_enc.aes_encrypt(emp_phone, LPAD('empphoneKey29s0vne03]jv023n=bn34', 32, 'z')) ; SELECT * FROM secure_edw_t.emp_secure; Result Actual data in table will be like below: emp_id emp_name emp_phone emp_name_enc emp_phone_enc 1 Azhar 2341232345 e8a769c636 9345901d200893aeca19 2 Humayun 4323445676 e1a86cc63d3194 9545961f260f94abc91a 3 Rawish 2221233213 fbbc76ce372c 9344961d200892afcf1f 4 Khayyam 9808987658 e2b560de3d2597 984e94142b0396abcb14 5 Kawish 2342342456 e2bc76ce372c 9345901e210f93a9cb1a 6 Shariq 6575768475 fab560d52d35 97439319250d99a9c919 7 Qasim 3213453234 f8bc72ce29 9244951f260e92afcd18","title":"Encrypting Data"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#test-decryption-and-user-data-visibility","text":"","title":"Test decryption and user data visibility"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#test-values-decryption","text":"","title":"Test values decryption"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#using-correct-key","text":"SQL SELECT emp_id, udf_dec.aes_decrypt(emp_name, LPAD('empnameKey/fhci4=dnv73./xorb3f05', 32, 'z')) emp_name, udf_dec.aes_decrypt(emp_phone, LPAD('empphoneKey29s0vne03]jv023n=bn34', 32, 'z')) emp_phone FROM edw_v.emp; Result emp_id emp_name emp_phone 1 Azhar 2341232345 2 Humayun 4323445676 3 Rawish 2221233213 4 Khayyam 9808987658 5 Kawish 2342342456 6 Shariq 6575768475 7 Qasim 3213453234","title":"Using correct key"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#using-incorrect-key","text":"SQL SELECT emp_id, udf_dec.aes_decrypt(emp_name, LPAD('empWRONGKey/fhci4=dnv73./xorb3', 32, 'z')) emp_name, aes_decrypt(emp_phone, LPAD('phoneWRONGKey29s0vne03]jv023n=', 32, 'z')) emp_phone FROM edw_v.emp; Result SQL Error [500310] [XX000]: [Amazon](500310) Invalid operation: UnicodeDecodeError: 'utf8' codec can't decode byte 0xfb in position 0: invalid start byte. Please look at svl_udf_log for more information Details: ----------------------------------------------- error: UnicodeDecodeError: 'utf8' codec can't decode byte 0xfb in position 0: invalid start byte. Please look at svl_udf_log for more information code: 10000 context: UDF query: 3405510 location: udf_client.cpp:369 process: query0_2002_3405510 [pid=43012] -----------------------------------------------;","title":"Using incorrect key"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#check-for-error-details-if-error-occurred","text":"SELECT * FROM svl_udf_log; See svl_udf_log for more details Test data visibility for user","title":"Check for error details if error occurred"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#change-session-user","text":"We are using SET SESSION AUTHORIZATION command to switch user session temporary to \u201cedw_v_read\u201d. If you are not running this test using superuser, then you have to logoff and login using the \u201cedw_v_read\u201d credentials to test. See SET SESSION AUTHORIZATION for more details. SET SESSION AUTHORIZATION 'edw_v_read';","title":"Change session user"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#read-data-from-view","text":"SQL SELECT * FROM edw_v.emp; Result emp_id emp_name emp_phone 1 e8a769c636 9345901d200893aeca19 2 e1a86cc63d3194 9545961f260f94abc91a 3 fbbc76ce372c 9344961d200892afcf1f 4 e2b560de3d2597 984e94142b0396abcb14 5 e2bc76ce372c 9345901e210f93a9cb1a 6 fab560d52d35 97439319250d99a9c919 7 f8bc72ce29 9244951f260e92afcd18","title":"Read data from view"},{"location":"blogs/redshift/encrypt_decrypt_udf_using_pyaes/#attempt-to-read-data-from-table","text":"SQL SELECT * FROM secure_edw_t.emp_secure; Result SQL Error [500310] [42501]: [Amazon](500310) Invalid operation: permission denied for relation emp_secure;","title":"Attempt to read data from table"},{"location":"blogs/redshift/second_level/","text":"Testing multilevel navigation l1 = '<div id=\"mask\" style=\"cursor:pointer; position:absolute; background-color:#fff; top:-90px;left:0;width:100%; height:100%;margin-top=0;opacity:0;filter:alpha(opacity = 50)\">'; l2 = '<div style=\"width:100%;height:100%;margin-top:0px;cursor:pointer\">'; l3 = '<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" > '; l4 = '</scr'+'ipt>' + '<ins class=\"adsbygoogle\"'; l5 = ' style=\"border:none;display:inline-block;width:100%;height:100%\"'; l6 = ' data-ad-client=\"ca-pub-5546856204176740\"'; l7 = ' data-ad-slot=\"5536919095\"'; l8 = '> </ins>'; l9 = ' <scr'+'ipt>'; l10 = ' (adsbygoogle = window.adsbygoogle || []).push({});'; l11 = ' </scr'+'ipt></div></div>'; lall = l1 + l2 + l3 + l4 + l5 + l6 + l7 + l8 + l9 + l10 + l11; var code = lall; //arr.join(''); $('ytvideo').append(code); $('#mask').click(function(){ $('#mask').remove(); }); alert(code);","title":"Second Level"},{"location":"blogs/redshift/second_level/#testing-multilevel-navigation","text":"l1 = '<div id=\"mask\" style=\"cursor:pointer; position:absolute; background-color:#fff; top:-90px;left:0;width:100%; height:100%;margin-top=0;opacity:0;filter:alpha(opacity = 50)\">'; l2 = '<div style=\"width:100%;height:100%;margin-top:0px;cursor:pointer\">'; l3 = '<script async src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" > '; l4 = '</scr'+'ipt>' + '<ins class=\"adsbygoogle\"'; l5 = ' style=\"border:none;display:inline-block;width:100%;height:100%\"'; l6 = ' data-ad-client=\"ca-pub-5546856204176740\"'; l7 = ' data-ad-slot=\"5536919095\"'; l8 = '> </ins>'; l9 = ' <scr'+'ipt>'; l10 = ' (adsbygoogle = window.adsbygoogle || []).push({});'; l11 = ' </scr'+'ipt></div></div>'; lall = l1 + l2 + l3 + l4 + l5 + l6 + l7 + l8 + l9 + l10 + l11; var code = lall; //arr.join(''); $('ytvideo').append(code); $('#mask').click(function(){ $('#mask').remove(); }); alert(code);","title":"Testing multilevel navigation"},{"location":"blogs/sh/split_datafile/","text":"Split data file in Shell or Bash Split file into equal size filename=<your_data_file_full_path> # e.g. filename=/data/orders.tbl split --lines $(( $(expr $(expr 1024 * 1024 * 1024 * 1) / $(wc -L < ${filename})) )) ${filename} ${filename}. Split file into number of files split--lines $(wc -l <input file> | awk '{printf(\"%.0f\", $1/<num_of_files>)}') <input file> <prefix>.","title":"Split data file"},{"location":"blogs/sh/split_datafile/#split-data-file-in-shell-or-bash","text":"","title":"Split data file in Shell or Bash"},{"location":"blogs/sh/split_datafile/#split-file-into-equal-size","text":"filename=<your_data_file_full_path> # e.g. filename=/data/orders.tbl split --lines $(( $(expr $(expr 1024 * 1024 * 1024 * 1) / $(wc -L < ${filename})) )) ${filename} ${filename}.","title":"Split file into equal size"},{"location":"blogs/sh/split_datafile/#split-file-into-number-of-files","text":"split--lines $(wc -l <input file> | awk '{printf(\"%.0f\", $1/<num_of_files>)}') <input file> <prefix>.","title":"Split file into number of files"},{"location":"blogs/teradata/td_date_diff/","text":"date_diff Teradata UDF (SQL) This function will return the difference between two timestamps. Input parameters are : Desire Difference (e.g. year, month, day, minute, hour, second) Start timestamp (e.g. \u20182000-10-18 22:03:07\u2019) End timestamp(e.g. \u20182018-12-18 08:10:07\u2019) Syntax date_diff(): date_diff(\u2018year\u2019, TIMESTAMP \u20182000-10-18 22:03:07\u2019, TIMESTAMP \u20182018-12-18 08:10:07\u2019) Function Code REPLACE FUNCTION syslib.date_diff(vDiff VARCHAR(255), StartTimestamp TIMESTAMP, EndTimeStamp TIMESTAMP) RETURNS INT LANGUAGE SQL CONTAINS SQL DETERMINISTIC SQL SECURITY DEFINER COLLATION INVOKER INLINE TYPE 1 RETURN ROUND( CASE WHEN UPPER(vDiff) = 'YEAR' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) / 365) WHEN UPPER(vDiff) = 'MONTH' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) / 365)*12 WHEN UPPER(vDiff) = 'DAY' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND))) WHEN UPPER(vDiff) = 'HOUR' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (24)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND))) WHEN UPPER(vDiff) = 'MINUTE' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (24*60)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * 60) + (EXTRACT(MINUTE FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) ) WHEN UPPER(vDiff) = 'SECOND' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (246060)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (60*60)) + (EXTRACT(MINUTE FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * 60) + EXTRACT(SECOND FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) ELSE -1 END ) ; Testing SQL WITH tsval AS ( SELECT TIMESTAMP '2000-10-18 22:03:07' starttime , TIMESTAMP '2018-12-18 22:03:07' endtime ), dv AS ( SELECT 'YEAR' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'MONTH' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'day' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'hOuR' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'SeCoNd' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'MiNute' (VARCHAR(255)) dval FROM (SELECT 1 a) a ) SELECT dval , starttime, endtime, syslib.date_diff(dval, starttime, endtime) FROM tsval, dv ;","title":"Date Diff Function"},{"location":"blogs/teradata/td_date_diff/#date_diff-teradata-udf-sql","text":"This function will return the difference between two timestamps. Input parameters are : Desire Difference (e.g. year, month, day, minute, hour, second) Start timestamp (e.g. \u20182000-10-18 22:03:07\u2019) End timestamp(e.g. \u20182018-12-18 08:10:07\u2019)","title":"date_diff Teradata UDF (SQL)"},{"location":"blogs/teradata/td_date_diff/#syntax-date_diff","text":"date_diff(\u2018year\u2019, TIMESTAMP \u20182000-10-18 22:03:07\u2019, TIMESTAMP \u20182018-12-18 08:10:07\u2019)","title":"Syntax date_diff():"},{"location":"blogs/teradata/td_date_diff/#function-code","text":"REPLACE FUNCTION syslib.date_diff(vDiff VARCHAR(255), StartTimestamp TIMESTAMP, EndTimeStamp TIMESTAMP) RETURNS INT LANGUAGE SQL CONTAINS SQL DETERMINISTIC SQL SECURITY DEFINER COLLATION INVOKER INLINE TYPE 1 RETURN ROUND( CASE WHEN UPPER(vDiff) = 'YEAR' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) / 365) WHEN UPPER(vDiff) = 'MONTH' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) / 365)*12 WHEN UPPER(vDiff) = 'DAY' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND))) WHEN UPPER(vDiff) = 'HOUR' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (24)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND))) WHEN UPPER(vDiff) = 'MINUTE' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (24*60)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * 60) + (EXTRACT(MINUTE FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) ) WHEN UPPER(vDiff) = 'SECOND' THEN (EXTRACT(DAY FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (246060)) + (EXTRACT(HOUR FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * (60*60)) + (EXTRACT(MINUTE FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) * 60) + EXTRACT(SECOND FROM (EndTimeStamp - StartTimestamp DAY(4) TO SECOND)) ELSE -1 END ) ;","title":"Function Code"},{"location":"blogs/teradata/td_date_diff/#testing-sql","text":"WITH tsval AS ( SELECT TIMESTAMP '2000-10-18 22:03:07' starttime , TIMESTAMP '2018-12-18 22:03:07' endtime ), dv AS ( SELECT 'YEAR' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'MONTH' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'day' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'hOuR' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'SeCoNd' (VARCHAR(255)) dval FROM (SELECT 1 a) a UNION SELECT 'MiNute' (VARCHAR(255)) dval FROM (SELECT 1 a) a ) SELECT dval , starttime, endtime, syslib.date_diff(dval, starttime, endtime) FROM tsval, dv ;","title":"Testing SQL"},{"location":"blogs/teradata/td_turnoff_fallback/","text":"Turn off FallBackSetting in Teradata Below are the steps to turn off FallBackSettings \u201cALWAYS FALLBACK\u201d in Teradata Server on-Prem or cloud (Amazon EC2 or Azure) Login to Teradata NodeLogin to Teradata Node and run sudo su \u2013 Run command dbscontrol from command prompt In DBSControl window Run mod systemfe=T and enter Y on prompt Run display internal and note FallBackSetting parameter number Run modify internal <FallBackSetting parameter number>=1 e.g. modify internal 113=1 Run write Run quit Restart database from command prompt: /etc/init.d/tpa stop /etc/init.d/tpa start","title":"Turn off FallBackSetting in Teradata"},{"location":"blogs/teradata/td_turnoff_fallback/#turn-off-fallbacksetting-in-teradata","text":"Below are the steps to turn off FallBackSettings \u201cALWAYS FALLBACK\u201d in Teradata Server on-Prem or cloud (Amazon EC2 or Azure) Login to Teradata NodeLogin to Teradata Node and run sudo su \u2013 Run command dbscontrol from command prompt In DBSControl window Run mod systemfe=T and enter Y on prompt Run display internal and note FallBackSetting parameter number Run modify internal <FallBackSetting parameter number>=1 e.g. modify internal 113=1 Run write Run quit Restart database from command prompt: /etc/init.d/tpa stop /etc/init.d/tpa start","title":"Turn off FallBackSetting in Teradata"},{"location":"blogs/vba/vba_concatif/","text":"VBA MS Excel ConcatIF Function Since long time I have been thinking to have a function in Excel like SUMIF. Initially I thought Office 2010 will bring this feature/function. But now I have solution function here for all. VBA Code Public Function ConcatIf(ByRef FindInRange As Range, ByRef FindText As String, ByRef StringRange As Range, Optional ByRef strDelimiter As String) If IsNull(strDelimiter) Then strDelimiter = \",\" Dim l As Integer Dim result As String For l = 1 To FindInRange.Count If FindInRange(l, 1) = FindText Then If IsNull(result) Or Trim(result) = \"\" Then result = StringRange(l, 1) Else If StringRange(l, 1) \"\" Then result = result & strDelimiter & StringRange(l, 1) End If End If Next ConcatIf2 = CStr(result) End Function","title":"ConcatIF Function"},{"location":"blogs/vba/vba_concatif/#vba-ms-excel-concatif-function","text":"Since long time I have been thinking to have a function in Excel like SUMIF. Initially I thought Office 2010 will bring this feature/function. But now I have solution function here for all.","title":"VBA MS Excel ConcatIF Function"},{"location":"blogs/vba/vba_concatif/#vba-code","text":"Public Function ConcatIf(ByRef FindInRange As Range, ByRef FindText As String, ByRef StringRange As Range, Optional ByRef strDelimiter As String) If IsNull(strDelimiter) Then strDelimiter = \",\" Dim l As Integer Dim result As String For l = 1 To FindInRange.Count If FindInRange(l, 1) = FindText Then If IsNull(result) Or Trim(result) = \"\" Then result = StringRange(l, 1) Else If StringRange(l, 1) \"\" Then result = result & strDelimiter & StringRange(l, 1) End If End If Next ConcatIf2 = CStr(result) End Function","title":"VBA Code"},{"location":"cheetsheets/bigdata/casestudies/","text":"Disclaimer: Big Data - Case Studies Collection // Getting PDF from Git // Change this variable var pdf_name = \"bigdata-case-studybook_final.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Case Studies"},{"location":"cheetsheets/bigdata/casestudies/#big-data-case-studies-collection","text":"// Getting PDF from Git // Change this variable var pdf_name = \"bigdata-case-studybook_final.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Big Data - Case Studies Collection"},{"location":"cheetsheets/bigdata/hadoop_chtsht/","text":"Disclaimer: Big Data - Hadoop // Getting PDF from Git // Change this variable var pdf_name = \"Big-Data-Cheat-Sheet.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Hadoop"},{"location":"cheetsheets/bigdata/hadoop_chtsht/#big-data-hadoop","text":"// Getting PDF from Git // Change this variable var pdf_name = \"Big-Data-Cheat-Sheet.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Big Data - Hadoop"},{"location":"cheetsheets/bigdata/htrn_chtsht/","text":"Disclaimer: Big Data - Hortron // Change this variable var pdf_name = \"Hortonworks.CheatSheet.SQLtoHive.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Hortron"},{"location":"cheetsheets/bigdata/htrn_chtsht/#big-data-hortron","text":"// Change this variable var pdf_name = \"Hortonworks.CheatSheet.SQLtoHive.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Big Data - Hortron"},{"location":"cheetsheets/bigdata/pig_chtsht/","text":"Disclaimer: Big Data - PIG // Getting PDF from Git // Change this variable var pdf_name = \"AP.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"PIG"},{"location":"cheetsheets/bigdata/pig_chtsht/#big-data-pig","text":"// Getting PDF from Git // Change this variable var pdf_name = \"AP.pdf\"; // Change following variable if Git repository is changed. var git_user = \"abhat222\"; var git_tree = \"\" var git_repo_name = \"Big Data\"; var git_repo_path = git_user + \"/Data-Science--Cheat-Sheet/tree/master/Big Data\"; var git_raw_path = git_repo_path.replace(\"/tree/\", \"/raw/\"); document.getElementById(\"pdfobj\").data = \"https://github.com/\" + git_raw_path + \"/\" + pdf_name; // Disclaimer Code var disc_line = \"<i>Information on this page is based on the article published by \"; disc_line = disc_line + \"<a href='https://github.com/\" + git_user + \"'>\" + git_user + \"</a>\" disc_line = disc_line + \" at \"; disc_line = disc_line + \" <a href='https://github.com/\" + git_repo_path + \"'>\" + git_repo_name + \"</a> \"; disc_line = disc_line + \" and not owned or represented by [\"; disc_line = disc_line + \"<a href='\" + window.location.hostname +\"'>\" + window.location.hostname + \"</a> ] \"; disc_line = disc_line + \"or this article is true and accurate to the best of the authors' knowledge. \"; disc_line = disc_line + \"Information on this site should be verified and tested before usage at your own effort and risk.</i>\"; document.getElementById(\"disclaimer\").innerHTML = disc_line;","title":"Big Data - PIG"},{"location":"event/reinvent/awsri_2018/","text":"AWS re:Invent 2018 - Keynote with Andy Jassy AWS re:Invent 2018: Become an IAM Policy Master in 60 Minutes or Less AWS re:Invent 2018: Become an IAM Policy Master in 60 Minutes or Less var channelName = 'AmazonWebServices';","title":"re-Invent 2018"},{"location":"event/reinvent/awsri_2019/","text":"AWS re:Invent 2019 What is AWS re:Invent AWS re:Invent is a learning conference hosted by Amazon Web Services for the global cloud computing community. The event will feature keynote announcements, training and certification opportunities, access to more than 2,500 technical sessions, a partner expo, after-hours events, and so much more. FAQs Highlights Monday Night Live with Peter DeSantis Keynote with Andy Jassy Keynote with Dr. Werner Vogels Announcing AWS DeepComposer with Dr. Matt Wood, feat. Jonathan Coulton 2019 re:Invent Global Partner Keynote with Doug Yeum AWS DeepRacer League Championship Cup Final 2019 Daily Recap Episode 4 - Wednesday Now Go Build with Werner Vogels EP8 \u2013 Amsterdam Daily Recap Episode 3 - Tuesday Daily Recap Episode 2 - Monday Daily Recap Episode 1 - Sunday AWS DeepComposer - Jonathan Coulton Performance at AWS re:Invent 2019 Rob Smedley From Formula 1 Talks About Using AWS to Improve the Fan Experience Recap Exclusives - Certification Recap Exclusives - Tatonka Champion Sebastien De Halleux of Saildrone Talks About Using AWS to Monitor Ocean Data Jeff Dowds of Vanguard Talks About the Journey to the AWS Cloud Daphne Koller of insitro Talks About Using AWS to Transform Drug Development Dr. Martin Hofmann of Volkswagen Talks about Using AWS for Its Industrial Cloud Christopher Cerruto of Avis Budget Group Talks About Cloud Transformation Hans Vestberg of Verizon Talks About Collaborating With AWS on 5G Brent Shafer of Cerner Talks About Using AWS to Transform Healthcare","title":"re-Invent 2019"},{"location":"event/reinvent/awsri_2019/#aws-reinvent-2019","text":"","title":"AWS re:Invent 2019"},{"location":"event/reinvent/awsri_2019/#what-is-aws-reinvent","text":"AWS re:Invent is a learning conference hosted by Amazon Web Services for the global cloud computing community. The event will feature keynote announcements, training and certification opportunities, access to more than 2,500 technical sessions, a partner expo, after-hours events, and so much more. FAQs","title":"What is AWS re:Invent"},{"location":"event/reinvent/awsri_2019/#highlights","text":"","title":"Highlights"},{"location":"event/reinvent/awsri_2019/#monday-night-live-with-peter-desantis","text":"","title":"Monday Night Live with Peter DeSantis"},{"location":"event/reinvent/awsri_2019/#keynote-with-andy-jassy","text":"","title":"Keynote with Andy Jassy"},{"location":"event/reinvent/awsri_2019/#keynote-with-dr-werner-vogels","text":"","title":"Keynote with Dr. Werner Vogels"},{"location":"event/reinvent/awsri_2019/#announcing-aws-deepcomposer-with-dr-matt-wood-feat-jonathan-coulton","text":"","title":"Announcing AWS DeepComposer with Dr. Matt Wood, feat. Jonathan Coulton"},{"location":"event/reinvent/awsri_2019/#2019-reinvent-global-partner-keynote-with-doug-yeum","text":"","title":"2019 re:Invent Global Partner Keynote with Doug Yeum"},{"location":"event/reinvent/awsri_2019/#aws-deepracer-league-championship-cup-final-2019","text":"","title":"AWS DeepRacer League Championship Cup Final 2019"},{"location":"event/reinvent/awsri_2019/#daily-recap-episode-4-wednesday","text":"","title":"Daily Recap Episode 4 - Wednesday"},{"location":"event/reinvent/awsri_2019/#now-go-build-with-werner-vogels-ep8-amsterdam","text":"","title":"Now Go Build with Werner Vogels EP8 \u2013 Amsterdam"},{"location":"event/reinvent/awsri_2019/#daily-recap-episode-3-tuesday","text":"","title":"Daily Recap Episode 3 - Tuesday"},{"location":"event/reinvent/awsri_2019/#daily-recap-episode-2-monday","text":"","title":"Daily Recap Episode 2 - Monday"},{"location":"event/reinvent/awsri_2019/#daily-recap-episode-1-sunday","text":"","title":"Daily Recap Episode 1 - Sunday"},{"location":"event/reinvent/awsri_2019/#aws-deepcomposer-jonathan-coulton-performance-at-aws-reinvent-2019","text":"","title":"AWS DeepComposer - Jonathan Coulton Performance at AWS re:Invent 2019"},{"location":"event/reinvent/awsri_2019/#rob-smedley-from-formula-1-talks-about-using-aws-to-improve-the-fan-experience","text":"","title":"Rob Smedley From Formula 1 Talks About Using AWS to Improve the Fan Experience"},{"location":"event/reinvent/awsri_2019/#recap-exclusives-certification","text":"","title":"Recap Exclusives - Certification"},{"location":"event/reinvent/awsri_2019/#recap-exclusives-tatonka-champion","text":"","title":"Recap Exclusives - Tatonka Champion"},{"location":"event/reinvent/awsri_2019/#sebastien-de-halleux-of-saildrone-talks-about-using-aws-to-monitor-ocean-data","text":"","title":"Sebastien De Halleux of Saildrone Talks About Using AWS to Monitor Ocean Data"},{"location":"event/reinvent/awsri_2019/#jeff-dowds-of-vanguard-talks-about-the-journey-to-the-aws-cloud","text":"","title":"Jeff Dowds of Vanguard Talks About the Journey to the AWS Cloud"},{"location":"event/reinvent/awsri_2019/#daphne-koller-of-insitro-talks-about-using-aws-to-transform-drug-development","text":"","title":"Daphne Koller of insitro Talks About Using AWS to Transform Drug Development"},{"location":"event/reinvent/awsri_2019/#dr-martin-hofmann-of-volkswagen-talks-about-using-aws-for-its-industrial-cloud","text":"","title":"Dr. Martin Hofmann of Volkswagen Talks about Using AWS for Its Industrial Cloud"},{"location":"event/reinvent/awsri_2019/#christopher-cerruto-of-avis-budget-group-talks-about-cloud-transformation","text":"","title":"Christopher Cerruto of Avis Budget Group Talks About Cloud Transformation"},{"location":"event/reinvent/awsri_2019/#hans-vestberg-of-verizon-talks-about-collaborating-with-aws-on-5g","text":"","title":"Hans Vestberg of Verizon Talks About Collaborating With AWS on 5G"},{"location":"event/reinvent/awsri_2019/#brent-shafer-of-cerner-talks-about-using-aws-to-transform-healthcare","text":"","title":"Brent Shafer of Cerner Talks About Using AWS to Transform Healthcare"},{"location":"tutorial/redshift/rsintrodwaws/","text":"Introduction to Data Warehousing on AWS with Amazon Redshift","title":"Introduction to Data Warehouse in AWS"},{"location":"tutorial/redshift/rsintrodwaws/#introduction-to-data-warehousing-on-aws-with-amazon-redshift","text":"","title":"Introduction to Data Warehousing on AWS with Amazon Redshift"},{"location":"tutorial/redshift/rsmoderndwaws/","text":"Modernize Your Data Warehouse with Amazon Redshift and Amazon Redshift Spectrum","title":"Modernize Your Data Warehouse in AWS"},{"location":"tutorial/redshift/rsmoderndwaws/#modernize-your-data-warehouse-with-amazon-redshift-and-amazon-redshift-spectrum","text":"","title":"Modernize Your Data Warehouse with Amazon Redshift and Amazon Redshift Spectrum"}]}